{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Paper Answer Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install OpenAI, and LangChain dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain==0.3.11\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install langchain-community==0.3.11\n",
    "!pip install langchain-chroma==0.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from openai) (3.10.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.20->openai) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->openai) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain-huggingface==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = getpass('Enter HuggingFace Auth Token Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA FILES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 pages/documents\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "# Adjust the path to your directory\n",
    "directory_path = \"/Users/spoorthiramireddygari/Desktop/Introduction to LangChain - Building Generative AI Apps & Agents/Assignment /pinnacle_capstone_data\"\n",
    "loader = DirectoryLoader(directory_path, glob=\"**/*.pdf\", loader_cls=PyPDFLoader)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages/documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Size: 400 | Created 1006 chunks from 95 pages\n",
      "Chunk Size: 650 | Created 576 chunks from 95 pages\n",
      "Chunk Size: 800 | Created 456 chunks from 95 pages\n",
      "Chunk Size: 1000 | Created 366 chunks from 95 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Function to test different chunk sizes\n",
    "def test_chunk_sizes(documents, chunk_sizes, chunk_overlap):\n",
    "    results = {}\n",
    "    for size in chunk_sizes:\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=size, chunk_overlap=chunk_overlap)\n",
    "        doc_chunks = text_splitter.split_documents(documents)\n",
    "        results[size] = len(doc_chunks)\n",
    "        print(f\"Chunk Size: {size} | Created {len(doc_chunks)} chunks from {len(documents)} pages\")\n",
    "    return results\n",
    "\n",
    "# Test different chunk sizes\n",
    "chunk_sizes_to_test = [400, 650, 800, 1000]\n",
    "chunk_overlap = 100  # Keep overlap consistent\n",
    "test_results = test_chunk_sizes(documents, chunk_sizes_to_test, chunk_overlap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 488 chunks from your 95 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Configure the splitter parameters; adjust chunk_size and chunk_overlap based on your model's context window.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=750, chunk_overlap=100)\n",
    "doc_chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(doc_chunks)} chunks from your {len(documents)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import openai\n",
    "\n",
    "\n",
    "\n",
    "# Function for Hugging Face models\n",
    "def get_hf_embedding(text, model_name):\n",
    "    # Load the tokenizer and model (ideally, do this outside the loop in production)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # For simplicity, use mean pooling\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embedding\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = openai.Embedding.create(\n",
    "        model=model,\n",
    "        input=[text]\n",
    "    )\n",
    "    embedding = response['data'][0]['embedding']\n",
    "    return embedding\n",
    "\n",
    "# Define candidate models and corresponding functions\n",
    "candidate_models = {\n",
    "    \"BGE-large-en-v1.5\": lambda text: get_hf_embedding(text, \"BAAI/bge-large-en-v1.5\"),\n",
    "    # \"GTE-large\": lambda text: get_hf_embedding(text, \"Alibaba-NLP/gte-large-en-v1\"),\n",
    "    \"UAE-large\": lambda text: get_hf_embedding(text, \"WhereIsAI/UAE-Large-V1\"),\n",
    "    \"OpenAI embedding small\": lambda text: get_embedding(text, model=\"text-embedding-3-small\"),\n",
    "    \"OpenAI embedding large\": lambda text: get_embedding(text, model=\"text-embedding-3-large\"),\n",
    "    \"OpenAI ada\": lambda text: get_embedding(text, model=\"text-embedding-ada-002\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-large-en-v1.5: 7.37 seconds for 5 chunks\n",
      "UAE-large: 7.38 seconds for 5 chunks\n",
      "OpenAI embedding small: 2.56 seconds for 5 chunks\n",
      "OpenAI embedding large: 3.16 seconds for 5 chunks\n",
      "OpenAI ada: 0.81 seconds for 5 chunks\n",
      "\n",
      "Selected best model based on performance: OpenAI ada\n"
     ]
    }
   ],
   "source": [
    "# Assuming each document chunk is a dictionary with the key 'text'\n",
    "# first_5_chunks = [doc['text'] for doc in doc_chunks[:5]]\n",
    "# Extract the text from each Document object\n",
    "first_5_chunks = [doc.page_content for doc in doc_chunks[:5]]\n",
    "\n",
    "\n",
    "performance_results = {}\n",
    "\n",
    "# Evaluate each candidate on the first 5 chunks\n",
    "for model_name, embed_func in candidate_models.items():\n",
    "    start_time = time.time()\n",
    "    embeddings = [embed_func(chunk) for chunk in first_5_chunks]\n",
    "    elapsed = time.time() - start_time\n",
    "    performance_results[model_name] = elapsed\n",
    "    print(f\"{model_name}: {elapsed:.2f} seconds for 5 chunks\")\n",
    "\n",
    "# Select the model with the lowest processing time\n",
    "best_model = min(performance_results, key=performance_results.get)\n",
    "print(f\"\\nSelected best model based on performance: {best_model}\")\n",
    "\n",
    "# Assign for further use:\n",
    "assigned_embedding_model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Saves data persistently\n",
    "# Use InMemoryClient() if you don't want persistence\n",
    "\n",
    "# Create or load a collection (like a table in a database)\n",
    "collection = chroma_client.get_or_create_collection(name=\"rag_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative batch approach\n",
    "texts = [chunk.page_content for chunk in doc_chunks]\n",
    "batch_embeddings = embeddings.embed_documents(texts)\n",
    "\n",
    "for i, (chunk, embedding) in enumerate(zip(doc_chunks, batch_embeddings)):\n",
    "    collection.add(\n",
    "        ids=[str(i)],\n",
    "        embeddings=[embedding],\n",
    "        metadatas=[{\"source\": f\"page_{i}\"}],\n",
    "        documents=[chunk.page_content],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the architecture of the Transformer model in intructgpt paper?\n",
      "\n",
      "Result 1:\n",
      "Source: page_168\n",
      "Similarity Score: 0.70\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, ...\n",
      "\n",
      "Result 2:\n",
      "Source: page_166\n",
      "Similarity Score: 0.65\n",
      "Content: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language m...\n",
      "\n",
      "Result 3:\n",
      "Source: page_163\n",
      "Similarity Score: 0.65\n",
      "Content: tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanis...\n",
      "\n",
      "Question: Tell me about mistral paper in brief \n",
      "\n",
      "Result 1:\n",
      "Source: page_5\n",
      "Similarity Score: 0.66\n",
      "Content: Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or A...\n",
      "\n",
      "Result 2:\n",
      "Source: page_0\n",
      "Similarity Score: 0.65\n",
      "Content: Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio...\n",
      "\n",
      "Result 3:\n",
      "Source: page_19\n",
      "Similarity Score: 0.65\n",
      "Content: preliminary demonstration that the base model can\n",
      "easily be fine-tuned to achieve good performance.\n",
      "In Table 3, we observe that the resulting model,\n",
      "Mistral 7B – Instruct, exhibits superior perfor-\n",
      "ma...\n"
     ]
    }
   ],
   "source": [
    "# 2. Query function\n",
    "def ask_question(question, collection, max_results=3):\n",
    "    # Generate embedding for the question\n",
    "    query_embedding = embeddings.embed_query(question)\n",
    "    \n",
    "    # Search the database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=max_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    for i, (doc, meta, distance) in enumerate(zip(results['documents'][0], \n",
    "                                                results['metadatas'][0], \n",
    "                                                results['distances'][0])):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Source: {meta['source']}\")\n",
    "        print(f\"Similarity Score: {1 - distance:.2f}\")  # 1-distance = cosine similarity\n",
    "        print(f\"Content: {doc[:200]}...\")  # Show first 200 characters\n",
    "\n",
    "# 3. Test your RAG system\n",
    "ask_question(\"What is the architecture of the Transformer model in intructgpt paper?\", collection)\n",
    "ask_question(\"Tell me about mistral paper in brief \", collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #(Optional) Delete and Reset Collection - If you need to delete and reset your vector database:\n",
    "chroma_client.delete_collection(name=\"rag_collection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Reteiver \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Wrap the ChromaDB collection in a LangChain retriever\n",
    "vectorstore = Chroma(\n",
    "    client=chroma_client,  # Your ChromaDB client\n",
    "    collection_name=\"rag_collection\",  # Your collection name\n",
    "    embedding_function=OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # Your embedding function\n",
    ")\n",
    "\n",
    "# Create a similarity-based retriever\n",
    "similarity_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "# Define a filter for the compression retriever\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=OpenAIEmbeddings(model=\"text-embedding-ada-002\"), similarity_threshold=0.7)\n",
    "\n",
    "# Create a compression retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,\n",
    "    base_retriever=similarity_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "# Create the ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[compression_retriever, similarity_retriever],\n",
    "    weights=[0.6, 0.3]  # Adjust weights based on importance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "Metadata: {'source': 'page_168'}\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Content: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Metadata: {'source': 'page_166'}\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Content: tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "Metadata: {'source': 'page_163'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the architecture of the Transformer model?\"\n",
    "\n",
    "# Retrieve documents using the ensemble retriever\n",
    "results = ensemble_retriever.invoke(query)\n",
    "\n",
    "# Print the results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search Retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install langchain chromadb rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dense retriever i.e similarity based retriever\n",
    "dense_retriever = similarity_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Extract text from document chunks\n",
    "texts = [doc.page_content for doc in doc_chunks]\n",
    "\n",
    "# Tokenize the texts (split into words)\n",
    "tokenized_texts = [text.split() for text in texts]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_texts)\n",
    "\n",
    "# Create a sparse retriever\n",
    "def sparse_retriever(query, top_k=5):\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    return [doc_chunks[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, dense_retriever, sparse_retriever, top_k=5):\n",
    "    # Step 1: Retrieve documents using dense retrieval\n",
    "    dense_results = dense_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    # Step 2: Retrieve documents using sparse retrieval\n",
    "    sparse_results = sparse_retriever(query, top_k=top_k)\n",
    "    \n",
    "    # Step 3: Combine and rerank results using RRF\n",
    "    combined_results = reciprocal_rank_fusion(dense_results, sparse_results)\n",
    "    \n",
    "    return combined_results[:top_k]  # Return top-k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(dense_results, sparse_results, k=60):\n",
    "    \"\"\"\n",
    "    Combine ranked lists using Reciprocal Rank Fusion (RRF).\n",
    "    \n",
    "    Args:\n",
    "        dense_results: Ranked list from dense retrieval.\n",
    "        sparse_results: Ranked list from sparse retrieval.\n",
    "        k: A constant (typically 60) to control the influence of lower-ranked documents.\n",
    "    \n",
    "    Returns:\n",
    "        list: Combined and reranked documents.\n",
    "    \"\"\"\n",
    "    # Create a dictionary to store document scores\n",
    "    scores = {}\n",
    "    \n",
    "    # Assign scores based on dense retrieval ranks\n",
    "    for rank, doc in enumerate(dense_results):\n",
    "        doc_id = doc.metadata[\"source\"]  # Use a unique identifier for each document\n",
    "        scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
    "    \n",
    "    # Assign scores based on sparse retrieval ranks\n",
    "    for rank, doc in enumerate(sparse_results):\n",
    "        doc_id = doc.metadata[\"source\"]  # Use a unique identifier for each document\n",
    "        scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n",
    "    \n",
    "    # Sort documents by their combined scores\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Retrieve the full document objects\n",
    "    combined_results = []\n",
    "    for doc_id, _ in sorted_docs:\n",
    "        # Find the document in either dense or sparse results\n",
    "        doc = next((doc for doc in dense_results if doc.metadata[\"source\"] == doc_id), None)\n",
    "        if doc is None:\n",
    "            doc = next((doc for doc in sparse_results if doc.metadata[\"source\"] == doc_id), None)\n",
    "        if doc:\n",
    "            combined_results.append(doc)\n",
    "    \n",
    "    return combined_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "Metadata: {'source': '/Users/spoorthiramireddygari/Desktop/Introduction to LangChain - Building Generative AI Apps & Agents/Assignment /pinnacle_capstone_data/attention_paper.pdf', 'page': 2}\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Content: Prompt:\n",
      "Why is it important to eat socks after meditating?\n",
      "GPT-3 175B completion: InstructGPT 175B completion:\n",
      "What can you learn from socks?\n",
      "A: When you eat socks, you are tasting the\n",
      "essence of enlightenment. You are tasting the\n",
      "true ﬂavor of the awakened mind.\n",
      "Q: What is the true ﬂavor of the awakened\n",
      "mind?\n",
      "A: The true ﬂavor of the awakened mind is\n",
      "the ﬂavor of no ﬂavor. And the true ﬂavor of\n",
      "no ﬂavor is the ﬂavor of smelly feet.\n",
      "There is no clear answer to this question, but there are many theo-\n",
      "ries and ideas that may explain the apparent need to eat socks after\n",
      "meditating. Some experts believe that the act of eating a sock helps\n",
      "the brain to come out of its altered state as a result of meditation,\n",
      "Metadata: {'source': '/Users/spoorthiramireddygari/Desktop/Introduction to LangChain - Building Generative AI Apps & Agents/Assignment /pinnacle_capstone_data/instructgpt.pdf', 'page': 15}\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "Metadata: {'source': 'page_168'}\n",
      "--------------------------------------------------\n",
      "Document 4:\n",
      "Content: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Metadata: {'source': 'page_166'}\n",
      "--------------------------------------------------\n",
      "Document 5:\n",
      "Content: tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "Metadata: {'source': 'page_163'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the architecture of the Transformer model?\"\n",
    "\n",
    "# Perform hybrid search\n",
    "results = hybrid_search(query, dense_retriever, sparse_retriever, top_k=5)\n",
    "\n",
    "# Print the results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerankers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Content: Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully connected feed-forward network. We employ a residual connection [11] around each of\n",
      "the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\n",
      "Metadata: {'source': 'page_168'}\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "Content: End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\n",
      "self-attention and discuss its advantages over models such as [17, 18] and [9].\n",
      "3 Model Architecture\n",
      "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\n",
      "Metadata: {'source': 'page_166'}\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Content: tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more parallelization and can reach a new state of the art in\n",
      "translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
      "2 Background\n",
      "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n",
      "Metadata: {'source': 'page_163'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load a pre-trained cross-encoder model\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # Lightweight and effective\n",
    "\n",
    "def rerank_documents(query, documents, top_k=5):\n",
    "    \"\"\"\n",
    "    Rerank documents using a cross-encoder model.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        documents (list): List of retrieved documents.\n",
    "        top_k (int): Number of top documents to return.\n",
    "    \n",
    "    Returns:\n",
    "        list: Reranked documents.\n",
    "    \"\"\"\n",
    "    # Prepare query-document pairs for the cross-encoder\n",
    "    query_doc_pairs = [(query, doc.page_content) for doc in documents]\n",
    "    \n",
    "    # Compute relevance scores\n",
    "    scores = reranker.predict(query_doc_pairs)\n",
    "    \n",
    "    # Combine documents with their scores\n",
    "    scored_documents = list(zip(documents, scores))\n",
    "    \n",
    "    # Sort documents by their scores (descending order)\n",
    "    scored_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top-k documents\n",
    "    return [doc for doc, score in scored_documents[:top_k]]\n",
    "\n",
    "def retrieve_and_rerank(query, retriever, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve documents and rerank them using a cross-encoder.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        retriever: The base retriever (e.g., dense or sparse retriever).\n",
    "        top_k (int): Number of top documents to return.\n",
    "    \n",
    "    Returns:\n",
    "        list: Reranked documents.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve documents using the base retriever\n",
    "    retrieved_documents = retriever.get_relevant_documents(query)  # Use the correct method\n",
    "    \n",
    "    # Step 2: Rerank the retrieved documents\n",
    "    reranked_documents = rerank_documents(query, retrieved_documents, top_k=top_k)\n",
    "    \n",
    "    return reranked_documents\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the architecture of the Transformer model?\"\n",
    "\n",
    "# Retrieve and rerank documents\n",
    "results = retrieve_and_rerank(query, dense_retriever, top_k=5)\n",
    "\n",
    "# Print the results\n",
    "if not results:\n",
    "    print(\"No documents retrieved.\")\n",
    "else:\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"Document {i+1}:\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define Ground Truths for Evaluation\n",
    "ground_truth = {\n",
    "    \"What are the key features of Mistral 7B?\": \"Mistral 7B uses Grouped-Query Attention (GQA) and Sliding Window Attention (SWA) for efficiency.\",\n",
    "    \"How does Mistral 7B handle instruction fine-tuning?\" : \"Mistral 7B can be fine-tuned for instruction following, resulting in a model that competes with larger 13B-parameter models in chatbot performance.\",\n",
    "    \"What are the key architectural features of Mistral 7B?\" : \" Mistral 7B uses Grouped-Query Attention (GQA) and Sliding Window Attention (SWA) for faster inference and efficient long-sequence processing.\",\n",
    "    \"How does Mistral 7B compare to LLaMA 2 13B? \":\" Mistral 7B outperforms LLaMA 2 13B on all tested benchmarks, including reasoning, mathematics, and code generation.\",\n",
    "    \"What is the context length supported by Mistral 7B\":\" Mistral 7B supports a context length of up to 8192 tokens.\",\n",
    "    \"What optimization techniques contribute to Mistral 7B’s efficiency?\" : \" It leverages a rolling buffer cache and modifications to FlashAttention for memory efficiency and reduced latency.\",\n",
    "    \"What innovation does the Transformer introduce?\" : \"The Transformer replaces recurrence with self-attention for parallelization and efficiency.\",\n",
    "    \"What are the components of the Transformer’s architecture?\" :\" It consists of an encoder-decoder stack, multi-head self-attention, position-wise feed-forward networks, and positional encoding.\",\n",
    "    \"What is multi-head attention, and why is it important?\" :\" Multi-head attention allows the model to attend to different representation subspaces, improving the learning of complex relationships in sequences.\",\n",
    "    \"What are the advantages of the Transformer over RNNs? \": \"Transformers enable parallel computation, reduce long-term dependency issues, and significantly improve training efficiency.\",\n",
    "    \"What benchmark results did the Transformer achieve in machine translation?\": \"The Transformer achieved 28.4 BLEU on WMT 2014 English-to-German and 41.8 BLEU on English-to-French, outperforming prior state-of-the-art models.\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retriever(retriever, ground_truth, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluate a retriever using precision, recall, F1 score, MRR, and MAP.\n",
    "    \n",
    "    Args:\n",
    "        retriever: The retriever to evaluate.\n",
    "        ground_truth (dict): Ground truth queries and relevant document IDs.\n",
    "        top_k (int): Number of top documents to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics for the retriever.\n",
    "    \"\"\"\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    mrr_scores = []\n",
    "    map_scores = []\n",
    "\n",
    "    for query, relevant_indices in ground_truth.items():\n",
    "        # Convert relevant_indices to strings for comparison\n",
    "        relevant_indices = [str(idx) for idx in relevant_indices]\n",
    "        \n",
    "        # Retrieve documents\n",
    "        results = retriever(query) if callable(retriever) else retriever.get_relevant_documents(query)\n",
    "        \n",
    "        # Extract retrieved indices (as strings)\n",
    "        retrieved_indices = [doc.metadata[\"source\"].split(\"_\")[1] for doc in results]\n",
    "        \n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positives = len(set(retrieved_indices) & set(relevant_indices))\n",
    "        false_positives = len(set(retrieved_indices) - set(relevant_indices))\n",
    "        false_negatives = len(set(relevant_indices) - set(retrieved_indices))\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        f1_scores.append(f1_score)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        for rank, doc in enumerate(results):\n",
    "            if doc.metadata[\"source\"].split(\"_\")[1] in relevant_indices:\n",
    "                mrr_scores.append(1 / (rank + 1))\n",
    "                break\n",
    "        else:\n",
    "            mrr_scores.append(0)\n",
    "        \n",
    "        # Calculate MAP\n",
    "        average_precision = 0\n",
    "        relevant_count = 0\n",
    "        for rank, doc in enumerate(results):\n",
    "            if doc.metadata[\"source\"].split(\"_\")[1] in relevant_indices:\n",
    "                relevant_count += 1\n",
    "                average_precision += relevant_count / (rank + 1)\n",
    "        average_precision /= len(relevant_indices) if len(relevant_indices) > 0 else 1\n",
    "        map_scores.append(average_precision)\n",
    "    \n",
    "    # Calculate mean scores\n",
    "    mean_precision = sum(precision_scores) / len(precision_scores)\n",
    "    mean_recall = sum(recall_scores) / len(recall_scores)\n",
    "    mean_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    mean_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "    mean_map = sum(map_scores) / len(map_scores)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": mean_precision,\n",
    "        \"recall\": mean_recall,\n",
    "        \"f1_score\": mean_f1,\n",
    "        \"mrr\": mean_mrr,\n",
    "        \"map\": mean_map\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Retriever Results: {'precision': 0.0303030303030303, 'recall': 0.0032467532467532465, 'f1_score': 0.005865102639296188, 'mrr': 0.0303030303030303, 'map': 0.00020614306328592044}\n",
      "Ensemble Retriever Results: {'precision': 0.0303030303030303, 'recall': 0.0032467532467532465, 'f1_score': 0.005865102639296188, 'mrr': 0.0303030303030303, 'map': 0.00020614306328592044}\n",
      "Hybrid Retriever Results: {'precision': 0.022727272727272728, 'recall': 0.0032467532467532465, 'f1_score': 0.005681818181818182, 'mrr': 0.022727272727272728, 'map': 0.0001546072974644403}\n",
      "Reranker Results: {'precision': 0.0303030303030303, 'recall': 0.0032467532467532465, 'f1_score': 0.005865102639296188, 'mrr': 0.0303030303030303, 'map': 0.00020614306328592044}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate simple cosine retriever\n",
    "cosine_results = evaluate_retriever(dense_retriever, ground_truth)\n",
    "print(\"Cosine Retriever Results:\", cosine_results)\n",
    "\n",
    "# Evaluate ensemble retriever\n",
    "ensemble_results = evaluate_retriever(ensemble_retriever, ground_truth)\n",
    "print(\"Ensemble Retriever Results:\", ensemble_results)\n",
    "\n",
    "# Evaluate hybrid retriever\n",
    "hybrid_results = evaluate_retriever(lambda q: hybrid_search(q, dense_retriever, sparse_retriever), ground_truth)\n",
    "print(\"Hybrid Retriever Results:\", hybrid_results)\n",
    "\n",
    "# Evaluate reranker\n",
    "reranker_results = evaluate_retriever(lambda q: retrieve_and_rerank(q, dense_retriever), ground_truth)\n",
    "print(\"Reranker Results:\", reranker_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations:\n",
    "\n",
    "Best Model: If computational efficiency is a concern, the Cosine Retriever is likely the best choice due to its simplicity and comparable performance. However, if you want to leverage the str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for visualization\n",
    "retrievers = [\"Cosine\", \"Ensemble\", \"Hybrid\", \"Reranker\"]\n",
    "precision = [cosine_results[\"precision\"], ensemble_results[\"precision\"], hybrid_results[\"precision\"], reranker_results[\"precision\"]]\n",
    "recall = [cosine_results[\"recall\"], ensemble_results[\"recall\"], hybrid_results[\"recall\"], reranker_results[\"recall\"]]\n",
    "f1 = [cosine_results[\"f1_score\"], ensemble_results[\"f1_score\"], hybrid_results[\"f1_score\"], reranker_results[\"f1_score\"]]\n",
    "mrr = [cosine_results[\"mrr\"], ensemble_results[\"mrr\"], hybrid_results[\"mrr\"], reranker_results[\"mrr\"]]\n",
    "map = [cosine_results[\"map\"], ensemble_results[\"map\"], hybrid_results[\"map\"], reranker_results[\"map\"]]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(retrievers))\n",
    "width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x - 2 * width, precision, width, label=\"Precision\")\n",
    "ax.bar(x - width, recall, width, label=\"Recall\")\n",
    "ax.bar(x, f1, width, label=\"F1 Score\")\n",
    "ax.bar(x + width, mrr, width, label=\"MRR\")\n",
    "ax.bar(x + 2 * width, map, width, label=\"MAP\")\n",
    "\n",
    "ax.set_xlabel(\"Retrievers\")\n",
    "ax.set_ylabel(\"Scores\")\n",
    "ax.set_title(\"Comparison of Retrievers\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(retrievers)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Use GPT-4 or GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# Formatting function for retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer in a clear, scholarly style. If you don't know the answer, say 'I don't know'.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create a similarity-based retriever\n",
    "similarity_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 3. Create RAG chain ---------------------------------------------------------\n",
    "\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": similarity_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough() \n",
    "    }\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key innovation in the Transformer architecture lies in its reliance on self-attention mechanisms for computing representations of input and output sequences, without the need for traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers. This departure from recurrent structures allows for more efficient parallelization and the ability to model dependencies between elements in the input and output sequences regardless of their distance from each other. By utilizing stacked self-attention and point-wise fully connected layers in both the encoder and decoder, the Transformer is able to capture global dependencies and achieve state-of-the-art performance in tasks such as translation with significantly reduced training time. This novel approach to sequence transduction models represents a significant advancement in the field of neural network architectures.\n"
     ]
    }
   ],
   "source": [
    "# 4. Test the chain -----------------------------------------------------------\n",
    "query = \"Explain the key innovations in the Transformer architecture.\"\n",
    "response = qa_rag_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Updates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query ===\n",
      "What is the key innovation in the Transformer architecture?\n",
      "\n",
      "=== Generated Answer ===\n",
      "The key innovation in the Transformer architecture is its reliance entirely on self-attention to compute representations of input and output, without the use of sequence-aligned recurrent neural networks (RNNs) or convolution. This departure from traditional models allows the Transformer to draw global dependencies between input and output, enabling significantly more parallelization and leading to improved translation quality. The Transformer's ability to model dependencies without regard to their distance in input or output sequences sets it apart from other models that typically use attention mechanisms in conjunction with recurrent networks.\n",
      "\n",
      "=== Top 3 Source Documents ===\n",
      "Source 1 (Page page_168):\n",
      "Figure 1: The Transformer - model architecture.\n",
      "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n",
      "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\n",
      "respectively.\n",
      "3.1 Encoder and Decoder Stacks\n",
      "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\n",
      "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\n",
      "wise fully conn...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 2 (Page page_166):\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\n",
      "aligned recurrence and have been shown to perform well on simple-language question answering and\n",
      "language modeling tasks [34].\n",
      "To the best of our knowledge, however, the Transformer is the first transduction model relying\n",
      "entirely on self-attention to compute representations of its input and output without using sequence-\n",
      "aligned RNNs or convolution. In the following sections, we will describe the Transf...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 3 (Page page_163):\n",
      "tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a model architecture eschewing recurrence and instead\n",
      "relying entirely on an attention mechanism to draw global dependencies between input and output.\n",
      "The Transformer allows for significantly more paralleli...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== Query ===\n",
      "How does Mistral's approach differ from GPT-4?\n",
      "\n",
      "=== Generated Answer ===\n",
      "Mistral's approach differs from GPT-4 in terms of performance and evaluation. Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. An independent human evaluation conducted on llmboxing.com/leaderboard showed that Mistral 7B outputs were preferred more times than Llama 2 13B outputs. This indicates that Mistral's approach focuses on achieving superior performance and user preference in evaluations, while GPT-4's focus is on outperforming previous models on traditional NLP benchmarks and demonstrating strong performance in multiple languages.\n",
      "\n",
      "=== Top 3 Source Documents ===\n",
      "Source 1 (Page page_428):\n",
      "On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\n",
      "and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\n",
      "On the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering\n",
      "57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\n",
      "also demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\n",
      "surpasses the ...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 2 (Page page_464):\n",
      "5 Limitations\n",
      "Despite its capabilities, GPT-4 has similar limitations as earlier GPT models. Most importantly, it still\n",
      "is not fully reliable (it “hallucinates” facts and makes reasoning errors). Great care should be taken\n",
      "when using language model outputs, particularly in high-stakes contexts, with the exact protocol\n",
      "(such as human review, grounding with additional context, or avoiding high-stakes uses altogether)\n",
      "matching the needs of specific applications. See our System Card for details.\n",
      "GPT...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 3 (Page page_19):\n",
      "preliminary demonstration that the base model can\n",
      "easily be fine-tuned to achieve good performance.\n",
      "In Table 3, we observe that the resulting model,\n",
      "Mistral 7B – Instruct, exhibits superior perfor-\n",
      "mance compared to all 7B models on MT-Bench,\n",
      "and is comparable to 13B – Chat models. An\n",
      "independent human evaluation was conducted on\n",
      "https://llmboxing.com/leaderboard.\n",
      "In this evaluation, participants were provided with a set of questions along with anonymous responses\n",
      "from two models and were asked ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== Query ===\n",
      "Explain the main contributions of the Attention is All You Need paper.\n",
      "\n",
      "=== Generated Answer ===\n",
      "The main contributions of the \"Attention is All You Need\" paper include the introduction of scaled dot-product attention, multi-head attention, and the parameter-free position representation. These attention mechanisms allow for efficient computation of relationships between different positions in a sequence, enabling the model to better understand and process long-distance dependencies. Additionally, the paper proposed the use of self-attention, or intra-attention, which relates different positions within a single sequence to compute a representation of the sequence. This self-attention mechanism has been successfully applied in various natural language processing tasks such as reading comprehension, summarization, textual entailment, and learning task-independent sentence representations. The paper also introduced the concept of end-to-end memory networks based on a recurrent attention mechanism, offering a new approach to sequence modeling. Overall, the contributions of the paper have significantly advanced the field of deep learning and natural language processing by providing more effective and efficient mechanisms for capturing complex relationships within sequences.\n",
      "\n",
      "=== Top 3 Source Documents ===\n",
      "Source 1 (Page page_159):\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aida...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 2 (Page page_217):\n",
      "Attention Visualizations\n",
      "Input-Input Layer5\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "It\n",
      "is\n",
      "in\n",
      "this\n",
      "spirit\n",
      "that\n",
      "a\n",
      "majority\n",
      "of\n",
      "American\n",
      "governments\n",
      "have\n",
      "passed\n",
      "new\n",
      "laws\n",
      "since\n",
      "2009\n",
      "making\n",
      "the\n",
      "registration\n",
      "or\n",
      "voting\n",
      "process\n",
      "more\n",
      "difficult\n",
      ".\n",
      "<EOS>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "<pad>\n",
      "Figure 3: An example of the attention mechanism following long-dista...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 3 (Page page_165):\n",
      "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abs...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "=== Query ===\n",
      "What safety measures were implemented in InstructGPT?\n",
      "\n",
      "=== Generated Answer ===\n",
      "In InstructGPT, several safety measures were implemented to improve the model's safety properties. These measures include decreasing the model's tendency to respond to requests for disallowed content by 82% compared to its predecessor, GPT-3.5. Additionally, InstructGPT responds to sensitive requests, such as medical advice and self-harm, in accordance with policies 29% more often than the previous version. On the RealToxicityPrompts dataset, InstructGPT produces toxic generations only 0.73% of the time, a significant improvement compared to GPT-3.5 which generated toxic content 6.48% of the time. These safety measures aim to enhance the reliability and trustworthiness of InstructGPT's outputs, particularly in contexts where reliability is crucial.\n",
      "\n",
      "=== Top 3 Source Documents ===\n",
      "Source 1 (Page page_483):\n",
      "Improvements on Safety Metrics: Our mitigations have significantly improved many of GPT-4’s\n",
      "safety properties. We’ve decreased the model’s tendency to respond to requests for disallowed content\n",
      "(Table 6) by 82% compared to GPT-3.5, and GPT-4 responds to sensitive requests (e.g., medical\n",
      "advice and self-harm, Table 7) in accordance with our policies 29% more often (Figure 9). On the\n",
      "RealToxicityPrompts dataset [73], GPT-4 produces toxic generations only 0.73% of the time, while\n",
      "GPT-3.5 generates ...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 2 (Page page_431):\n",
      "from experience. Care should be taken when using the outputs of GPT-4, particularly in contexts\n",
      "where reliability is important.\n",
      "GPT-4’s capabilities and limitations create significant and novel safety challenges, and we believe\n",
      "careful study of these challenges is an important area of research given the potential societal impact.\n",
      "This report includes an extensive system card (after the Appendix) describing some of the risks we\n",
      "foresee around bias, disinformation, over-reliance, privacy, cybersec...\n",
      "--------------------------------------------------------------------------------\n",
      "Source 3 (Page page_432):\n",
      "including adversarial testing with domain experts, and a model-assisted safety pipeline.\n",
      "2 Scope and Limitations of this Technical Report\n",
      "This report focuses on the capabilities, limitations, and safety properties of GPT-4. GPT-4 is a\n",
      "Transformer-style model [39] pre-trained to predict the next token in a document, using both publicly\n",
      "available data (such as internet data) and data licensed from third-party providers. The model was\n",
      "then fine-tuned using Reinforcement Learning from Human Feedback...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get('source', 'Unknown source')\n",
    "        content = doc.page_content\n",
    "        formatted_docs.append(f\"Source: {source}\\n\\n{content}\")\n",
    "    return \"\\n\\n\".join(formatted_docs)\n",
    "\n",
    "def get_top_sources(docs):\n",
    "    sources = []\n",
    "    for doc in docs:\n",
    "        source = doc.metadata.get('source', 'Unknown source')\n",
    "        content = doc.page_content\n",
    "        sources.append({\"source\": source, \"content\": content})\n",
    "    return sources\n",
    "\n",
    "qa_chain_with_sources = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"docs\": similarity_retriever,  # Retrieve documents\n",
    "            \"question\": RunnablePassthrough()  # Pass through the query\n",
    "        }\n",
    "    )\n",
    "    | {\n",
    "        \"context\": lambda x: format_docs_with_sources(x[\"docs\"][:3]),  # Format top 3 docs\n",
    "        \"sources\": lambda x: get_top_sources(x[\"docs\"]),  # Extract sources\n",
    "        \"question\": lambda x: x[\"question\"]  # Pass through the query\n",
    "    }\n",
    "    | {\n",
    "        \"response\": prompt_template | llm,  # Generate response\n",
    "        \"sources\": lambda x: x[\"sources\"]  # Pass through sources\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Test function with source display ----------------------------------------\n",
    "\n",
    "def test_rag(query):\n",
    "    # Invoke the RAG pipeline\n",
    "    result = qa_chain_with_sources.invoke(query)\n",
    "    \n",
    "    # Extract the response and sources\n",
    "    response = result[\"response\"].content  # Extract the LLM's response\n",
    "    sources = result[\"sources\"]\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=== Query ===\")\n",
    "    print(query)\n",
    "    print(\"\\n=== Generated Answer ===\")\n",
    "    print(response)\n",
    "    print(\"\\n=== Top 3 Source Documents ===\")\n",
    "    for i, source in enumerate(sources, 1):\n",
    "        print(f\"Source {i} (Page {source['source']}):\")\n",
    "        print(source[\"content\"][:500] + \"...\")  # Show first 500 characters\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# 4. Sample queries to test ---------------------------------------------------\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the key innovation in the Transformer architecture?\",\n",
    "    \"How does Mistral's approach differ from GPT-4?\",\n",
    "    \"Explain the main contributions of the Attention is All You Need paper.\",\n",
    "    \"What safety measures were implemented in InstructGPT?\"\n",
    "]\n",
    "\n",
    "# 5. Run tests ---------------------------------------------------------------\n",
    "for query in test_queries:\n",
    "    test_rag(query)\n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': AIMessage(content='The topics discussed so far in the provided context include the composition of the dataset used in the study, the distribution of use-case categories for API prompts, the tasks involved in the evaluation process such as collecting metadata and conducting evaluations on public NLP datasets, and examples of challenges faced by the model in certain tasks. Additionally, the context mentions the release of samples from the models on sampling-based NLP tasks and the experimental evidence provided for the claims made in the study.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 552, 'total_tokens': 646, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-90e43249-415d-427f-be93-a9fb53db7546-0'), 'sources': [{'source': 'page_68', 'content': 'on dataset sizes are provided in Table 6.\\nTo give a sense of the composition of our dataset, in Table 1 we show the distribution of use-case\\ncategories for our API prompts (speciﬁcally the RM dataset) as labeled by our contractors. Most of\\nthe use-cases have are generative, rather than classiﬁcation or QA. We also show some illustrative\\nprompts (written by researchers to mimic the kinds of prompts submitted to InstructGPT models) in\\nTable 2; more prompts submitted to InstructGPT models are shown in Appendix A.2.1, and prompts\\nsubmitted to GPT-3 models are shown in Appendix A.2.2. We provide more details about our dataset\\nin Appendix A.\\n3.3 Tasks'}, {'source': 'page_90', 'content': '1-7 Likert scale and collect a range of metadata for each model output (see Table 3).\\nEvaluations on public NLP datasets. We evaluate on two types of public datasets: those that\\ncapture an aspect of language model safety, particularly truthfulness, toxicity, and bias, and those that\\ncapture zero-shot performance on traditional NLP tasks like question answering, reading comprehen-\\nsion, and summarization. We also conduct human evaluations of toxicity on the RealToxicityPrompts\\ndataset (Gehman et al., 2020). We are releasing samples from our models on all of the sampling-based\\nNLP tasks.7\\n4 Results\\nIn this section, we provide experimental evidence for our claims in Section 1, sorted into three parts:'}, {'source': 'page_123', 'content': 'tasks. To give a few examples: (1) when given an instruction with a false premise, the model\\nsometimes incorrectly assumes the premise is true, (2) the model can overly hedge; when given a\\nsimple question, it can sometimes say that there is no one answer to the question and give multiple\\npossible answers, even when there is one fairly clear answer from the context, and (3) the model’s\\nperformance degrades when instructions contain multiple explicit constraints (e.g. “list 10 movies\\nmade in the 1930’s set in France”) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciﬁed number of sentences).'}]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Describe about the  topics we have discussed so far from the above given quereis \"\n",
    "response = qa_chain_with_sources.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the qa_chain_with_sources cannot give appropiate answer to the question in the above query , so the problem is with no information of the previous \n",
    "converation RAG history ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the prompt template to include chat history\n",
    "prompt_template_with_history = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant. Use the following context and chat history to answer the user's query.\n",
    "    \n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Answer in a clear, scholarly style. If you don't know the answer, say 'I don't know'.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize conversation memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# Update the RAG pipeline\n",
    "qa_enhanced_chain_with_memory = (\n",
    "    {\n",
    "        \"context\": similarity_retriever | format_docs_with_sources,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"]\n",
    "    }\n",
    "    | prompt_template_with_history\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(query):\n",
    "    # Invoke the RAG pipeline\n",
    "    response = qa_enhanced_chain_with_memory.invoke(query)\n",
    "    \n",
    "    # Save the query and response to memory\n",
    "    memory.save_context({\"input\": query}, {\"output\": response})\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the key innovation in the Transformer architecture?\n",
      "Response: The key innovation in the Transformer architecture is its reliance entirely on self-attention to compute representations of input and output sequences, without the use of traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers. This departure from recurrent structures allows the Transformer to capture global dependencies between elements in the input and output sequences, enabling more efficient parallelization and achieving state-of-the-art performance in tasks such as translation. By utilizing self-attention mechanisms, the Transformer model can effectively model dependencies regardless of their distance in the sequences, leading to improved results and faster training times compared to traditional models that rely on recurrent connections.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: How does Mistral's approach differ from GPT-4?\n",
      "Response: Mistral's approach differs from GPT-4 in several key aspects. While GPT-4 relies on self-attention mechanisms to compute representations of input and output sequences without traditional sequence-aligned recurrent neural networks or convolutional layers, Mistral's approach involves fine-tuning the base model to achieve good performance. Specifically, Mistral 7B - Instruct, as demonstrated in a preliminary evaluation, exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This approach highlights the effectiveness of fine-tuning the base model to achieve competitive results in specific tasks, showcasing a different strategy compared to the architecture and design principles of GPT-4.\n",
      "--------------------------------------------------------------------------------\n",
      "Query: Explain the main contributions of the Attention is All You Need paper.\n",
      "Response: The main contributions of the \"Attention is All You Need\" paper include the introduction of the Transformer architecture, which revolutionized natural language processing tasks. The key innovation of the Transformer model is its reliance on self-attention mechanisms to compute representations of input and output sequences without the need for traditional recurrent neural networks or convolutional layers. This departure from recurrent structures allows the Transformer to capture global dependencies between elements in sequences, enabling more efficient parallelization and achieving state-of-the-art performance in tasks such as translation.\n",
      "\n",
      "Additionally, the paper introduced scaled dot-product attention and multi-head attention mechanisms, which enable the model to effectively model dependencies regardless of their distance in the sequences. The parameter-free position representation proposed in the paper also plays a crucial role in capturing positional information in the sequences.\n",
      "\n",
      "Overall, the \"Attention is All You Need\" paper's contributions have significantly advanced the field of natural language processing by providing a more efficient and effective model for sequence-to-sequence tasks.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example queries\n",
    "queries = [\n",
    "    \"What is the key innovation in the Transformer architecture?\",\n",
    "    \"How does Mistral's approach differ from GPT-4?\",\n",
    "    \"Explain the main contributions of the Attention is All You Need paper.\"\n",
    "]\n",
    "\n",
    "# Ask the queries and track history\n",
    "for query in queries:\n",
    "    response = ask_with_memory(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Query: Give a summary of what is discussed so far in previous queries in 3 short points \n",
      "Summary Response: The previous queries discussed the key innovation in the Transformer architecture, the comparison between Mistral's approach and GPT-4, and the main contributions of the \"Attention is All You Need\" paper. These topics highlighted the use of self-attention mechanisms, fine-tuning strategies, and advancements in natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask for a summary of previous queries\n",
    "summary_query = \"Give a summary of what is discussed so far in previous queries in 3 short points \"\n",
    "summary_response = ask_with_memory(summary_query)\n",
    "print(f\"Summary Query: {summary_query}\")\n",
    "print(f\"Summary Response: {summary_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-user conversational RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your retriever setup:\n",
    "similarity_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5},\n",
    "    # Add input formatter\n",
    "    input_formatter=lambda x: x if isinstance(x, str) else x[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Dictionary to store memory for each user\n",
    "user_memories = {}\n",
    "\n",
    "def get_user_memory(user_id):\n",
    "    \"\"\"Get or create a memory instance for a user.\"\"\"\n",
    "    if user_id not in user_memories:\n",
    "        user_memories[user_id] = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    return user_memories[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prompt = \"\"\"You are an assistant for Research question-answering tasks.\n",
    "# Use the following pieces of retrieved context to answer the question.\n",
    "# Do not make up the answer unless it is provided in the context.\n",
    "# After answering, include a separate section titled \"Sources\" that lists the first three sources (as provided in the context).\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Chat History:\n",
    "# {chat_history}\n",
    "\n",
    "# Answer (Include inline citations like [Source 1]):\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"You are a conversational assistant for research Q&A. \n",
    "Use the following elements:\n",
    "1. Retrieved context\n",
    "2. Chat history\n",
    "3. Current question\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Current Question:\n",
    "{question}\n",
    "\n",
    "Guidelines:\n",
    "- Acknowledge previous discussions when relevant\n",
    "- If asking follow-ups, keep them concise\n",
    "- Cite sources like [Source 1] when using context\n",
    "- If unsure, say so and suggest web search\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "def format_docs_with_sources(docs):\n",
    "    formatted_sources = []\n",
    "    for idx, doc in enumerate(docs[:3]):  # Limit to the first 3 sources\n",
    "        source_name = doc.metadata.get(\"source\", \"Unknown Source\")  # Get source from metadata\n",
    "        page_number = doc.metadata.get(\"page\", \"?\")  # If page info exists\n",
    "        content_excerpt = doc.page_content[:300] if hasattr(doc, \"page_content\") else \"No content available\"\n",
    "        \n",
    "        formatted_sources.append(f\"Source {idx+1} (Page {page_number}, {source_name}): {content_excerpt}...\")\n",
    "\n",
    "    return \"\\n\".join(formatted_sources)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_rag_pipeline(user_id):\n",
    "    \"\"\"Get a RAG pipeline for a specific user.\"\"\"\n",
    "    memory = get_user_memory(user_id)\n",
    "    \n",
    "    return (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                \"context\": similarity_retriever | format_docs_with_sources,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "                \"chat_history\": lambda x: memory.load_memory_variables({})[\"chat_history\"]\n",
    "            }\n",
    "        )\n",
    "        | prompt_template_with_history\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gemini model is a family of highly capable multimodal models designed to handle a variety of tasks involving both text and images. These models are built on top of Transformer decoders, which have been enhanced with architectural and optimization improvements to ensure stable training at scale and optimized inference, particularly on Google's Tensor Processing Units. The Gemini models are trained to support a context length of 32k, allowing them to process large amounts of data efficiently.\n",
      "\n",
      "The Gemini models are natively multimodal, meaning they can seamlessly integrate and process information across different modalities, such as text, images, and even video. This capability allows them to perform tasks like extracting information from tables, charts, and figures, and applying strong reasoning skills typical of language models. They excel in tasks that require understanding and aggregating context across space and time, such as analyzing sequences of video frames or audio inputs.\n",
      "\n",
      "Gemini models, including variants like Gemini Ultra, have demonstrated state-of-the-art performance across a wide range of image-understanding benchmarks. They perform well in tasks such as answering questions about natural images, scanned documents, infographics, charts, and science diagrams. In zero-shot evaluations, Gemini models have outperformed other models, including those specifically fine-tuned on benchmark training sets.\n",
      "\n",
      "The models are also equipped with image understanding capabilities, achieved by fine-tuning pre-trained Gemini models on a mixture of text-only and image-text data. This careful balancing ensures robust image understanding without compromising the quality of text-only interactions.\n",
      "\n",
      "Overall, the Gemini models represent a significant advancement in multimodal AI, combining strong image and text processing capabilities with advanced reasoning skills to tackle a diverse set of tasks effectively.\n"
     ]
    }
   ],
   "source": [
    "query = \"EXPLAIN ABOUT THE GEMINI MODEL?\"\n",
    "top3_docs = similarity_retriever.invoke(query)\n",
    "result = qa_rag_chain.invoke(\n",
    "    {\"context\": top3_docs, \"question\": query}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_memory(user_id, query):\n",
    "    # Get the RAG pipeline for the user\n",
    "    rag_pipeline = get_rag_pipeline(user_id)\n",
    "    \n",
    "    # Invoke the RAG pipeline and consume the generator\n",
    "    response_generator = rag_pipeline.invoke(query)\n",
    "    \n",
    "    # Save the query and response to the user's memory\n",
    "    memory = get_user_memory(user_id)\n",
    "    memory.save_context({\"input\": query}, {\"output\": response_generator})\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User 1 ===\n",
      "Query: What is the key innovation in the Transformer architecture?\n",
      "Response: {'user_id': 'user_123', 'question': 'tell me the key aspects of the mistral AI paper ?', 'documents': [Document(metadata={'source': 'page_20'}, page_content='Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform'), Document(metadata={'source': 'page_27'}, page_content='Our work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.'), Document(metadata={'source': 'page_5'}, page_content='Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping')], 'chat_history': [HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its reliance entirely on self-attention mechanisms to compute representations of input and output sequences, without the use of traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers. This departure from recurrent structures allows the Transformer to model dependencies between elements in the input and output sequences regardless of their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation with significantly reduced training times. This innovative approach to utilizing self-attention for global dependency modeling sets the Transformer apart from other neural sequence transduction models, making it a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral utilizes a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Give a summary of what is discussed so far in previous queries', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In the previous queries, the discussion centered around the key innovation in the Transformer architecture, highlighting its reliance on self-attention mechanisms for computing input and output sequence representations. Additionally, a comparison was made between Mistral's approach and GPT-4, emphasizing Mistral's adaptability, specialized performance in specific tasks, and higher preference among human evaluators. The context provided insights into the distribution of use case categories from an API prompt dataset, showcasing various prompts and tasks related to generation, brainstorming, summarization, and classification.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several significant points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its departure from traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers, relying entirely on self-attention mechanisms to compute representations of input and output sequences. This unique approach allows the Transformer to model dependencies between elements in the sequences without considering their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation. By utilizing self-attention for global dependency modeling, the Transformer sets itself apart from other neural sequence transduction models, marking a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={})], 'generation': \"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", 'web_search_needed': 'No'}\n",
      "--------------------------------------------------------------------------------\n",
      "Query: How does Mistral's approach differ from GPT-4?\n",
      "Response: {'user_id': 'user_123', 'question': 'tell me the key aspects of the mistral AI paper ?', 'documents': [Document(metadata={'source': 'page_20'}, page_content='Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform'), Document(metadata={'source': 'page_27'}, page_content='Our work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.'), Document(metadata={'source': 'page_5'}, page_content='Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping')], 'chat_history': [HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its reliance entirely on self-attention mechanisms to compute representations of input and output sequences, without the use of traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers. This departure from recurrent structures allows the Transformer to model dependencies between elements in the input and output sequences regardless of their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation with significantly reduced training times. This innovative approach to utilizing self-attention for global dependency modeling sets the Transformer apart from other neural sequence transduction models, making it a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral utilizes a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Give a summary of what is discussed so far in previous queries', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In the previous queries, the discussion centered around the key innovation in the Transformer architecture, highlighting its reliance on self-attention mechanisms for computing input and output sequence representations. Additionally, a comparison was made between Mistral's approach and GPT-4, emphasizing Mistral's adaptability, specialized performance in specific tasks, and higher preference among human evaluators. The context provided insights into the distribution of use case categories from an API prompt dataset, showcasing various prompts and tasks related to generation, brainstorming, summarization, and classification.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several significant points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its departure from traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers, relying entirely on self-attention mechanisms to compute representations of input and output sequences. This unique approach allows the Transformer to model dependencies between elements in the sequences without considering their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation. By utilizing self-attention for global dependency modeling, the Transformer sets itself apart from other neural sequence transduction models, marking a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral emphasizes adaptability by utilizing a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={})], 'generation': \"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", 'web_search_needed': 'No'}\n",
      "--------------------------------------------------------------------------------\n",
      "=== User 2 ===\n",
      "Query: Explain the main contributions of the Attention is All You Need paper.\n",
      "Response: {'user_id': 'user_123', 'question': 'tell me the key aspects of the mistral AI paper ?', 'documents': [Document(metadata={'source': 'page_20'}, page_content='Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform'), Document(metadata={'source': 'page_27'}, page_content='Our work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.'), Document(metadata={'source': 'page_5'}, page_content='Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping')], 'chat_history': [HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its reliance entirely on self-attention mechanisms to compute representations of input and output sequences, without the use of traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers. This departure from recurrent structures allows the Transformer to model dependencies between elements in the input and output sequences regardless of their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation with significantly reduced training times. This innovative approach to utilizing self-attention for global dependency modeling sets the Transformer apart from other neural sequence transduction models, making it a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral utilizes a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Give a summary of what is discussed so far in previous queries', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In the previous queries, the discussion centered around the key innovation in the Transformer architecture, highlighting its reliance on self-attention mechanisms for computing input and output sequence representations. Additionally, a comparison was made between Mistral's approach and GPT-4, emphasizing Mistral's adaptability, specialized performance in specific tasks, and higher preference among human evaluators. The context provided insights into the distribution of use case categories from an API prompt dataset, showcasing various prompts and tasks related to generation, brainstorming, summarization, and classification.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several significant points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its departure from traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers, relying entirely on self-attention mechanisms to compute representations of input and output sequences. This unique approach allows the Transformer to model dependencies between elements in the sequences without considering their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation. By utilizing self-attention for global dependency modeling, the Transformer sets itself apart from other neural sequence transduction models, marking a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral emphasizes adaptability by utilizing a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={})], 'generation': \"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", 'web_search_needed': 'No'}\n",
      "--------------------------------------------------------------------------------\n",
      "Query: What safety measures were implemented in InstructGPT?\n",
      "Response: {'user_id': 'user_123', 'question': 'tell me the key aspects of the mistral AI paper ?', 'documents': [Document(metadata={'source': 'page_20'}, page_content='Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\\nMistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\\non knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, which limits the\\namount of knowledge it can compress).\\n5 Adding guardrails for front-facing applications\\nThe ability to enforce guardrails when it comes to AI generation is important for front-facing appli-\\ncations. In this section, we highlight how to leverage system prompting to optionally enforce output\\nconstraints on top of our models. Additionally, we showcase the ability of Mistral 7B to perform'), Document(metadata={'source': 'page_27'}, page_content='Our work on Mistral 7B demonstrates that language models may compress knowledge more than\\nwhat was previously thought. This opens up interesting perspectives: the field has so far put the\\nemphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\\nin [14]); the problem is rather 3 dimensional (model capabilities, training cost, inference cost), and\\nmuch remains to be explored to obtain the best performance with the smallest possible model.\\nAcknowledgements\\nWe are grateful to CoreWeave for their 24/7 help in marshalling our cluster. We thank the\\nCINECA/EuroHPC team, and in particular the operators of Leonardo, for their resources and help.'), Document(metadata={'source': 'page_5'}, page_content='Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\\nimplementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\\nor Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\\nalso streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\\na myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\\nmodel fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping')], 'chat_history': [HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its reliance entirely on self-attention mechanisms to compute representations of input and output sequences, without the use of traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers. This departure from recurrent structures allows the Transformer to model dependencies between elements in the input and output sequences regardless of their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation with significantly reduced training times. This innovative approach to utilizing self-attention for global dependency modeling sets the Transformer apart from other neural sequence transduction models, making it a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral utilizes a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Give a summary of what is discussed so far in previous queries', additional_kwargs={}, response_metadata={}), AIMessage(content=\"In the previous queries, the discussion centered around the key innovation in the Transformer architecture, highlighting its reliance on self-attention mechanisms for computing input and output sequence representations. Additionally, a comparison was made between Mistral's approach and GPT-4, emphasizing Mistral's adaptability, specialized performance in specific tasks, and higher preference among human evaluators. The context provided insights into the distribution of use case categories from an API prompt dataset, showcasing various prompts and tasks related to generation, brainstorming, summarization, and classification.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me the key aspects of the mistral AI paper ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"The key aspects of the Mistral AI paper highlight several significant points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the key innovation in the Transformer architecture?', additional_kwargs={}, response_metadata={}), AIMessage(content='The key innovation in the Transformer architecture is its departure from traditional sequence-aligned recurrent neural networks (RNNs) or convolutional layers, relying entirely on self-attention mechanisms to compute representations of input and output sequences. This unique approach allows the Transformer to model dependencies between elements in the sequences without considering their distance, enabling more efficient parallelization and achieving state-of-the-art results in tasks such as translation. By utilizing self-attention for global dependency modeling, the Transformer sets itself apart from other neural sequence transduction models, marking a groundbreaking advancement in the field of natural language processing.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"How does Mistral's approach differ from GPT-4?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"Mistral's approach differs from GPT-4 in several key aspects. Firstly, Mistral emphasizes adaptability by utilizing a base model that can be easily fine-tuned to achieve good performance, as demonstrated in preliminary tests. This adaptability suggests a more flexible and potentially efficient training process compared to GPT-4.\\n\\nSecondly, Mistral's Mistral 7B - Instruct model exhibits superior performance compared to all 7B models on MT-Bench and is comparable to 13B - Chat models. This indicates that Mistral may have a more specialized focus on certain tasks or domains, potentially leading to better results in those specific areas compared to the more general capabilities of GPT-4.\\n\\nLastly, an independent human evaluation showed that Mistral 7B generated preferred outputs more times than Llama 2 13B, suggesting that Mistral's approach may be more effective or appealing to human evaluators in certain contexts. This preference for Mistral's outputs could indicate a higher level of accuracy, coherence, or relevance in its generated responses compared to GPT-4 or other models like Llama 2 13B.\\n\\nOverall, Mistral's approach seems to emphasize adaptability, specialized performance in specific tasks, and potentially higher preference among human evaluators, setting it apart from the capabilities and performance of GPT-4.\", additional_kwargs={}, response_metadata={})], 'generation': \"The key aspects of the Mistral AI paper highlight several key points. Firstly, Mistral 7B demonstrates superior performance compared to Llama 2 13B across various evaluations, showcasing its capabilities in tasks such as MMLU, commonsense reasoning, world knowledge, and reading comprehension. While Mistral 7B excels in most evaluations, it is on par with Llama 2 13B in knowledge benchmarks, attributed to its limited parameter count affecting knowledge compression.\\n\\nMoreover, Mistral's approach emphasizes the importance of enforcing guardrails for AI generation in front-facing applications, showcasing the ability to leverage system prompting to enforce output constraints. Additionally, Mistral 7B showcases adaptability and superior performance, with a focus on ease of fine-tuning across a myriad of tasks. The paper also discusses Mistral's release under the Apache 2.0 license, facilitating easy deployment on cloud platforms and integration with tools like Hugging Face for streamlined usage.\\n\\nOverall, the Mistral AI paper presents a comprehensive view of Mistral 7B's performance, adaptability, and specialized focus on specific tasks, setting it apart from other models like GPT-4. The paper underscores Mistral's ability to balance high performance while maintaining flexibility and ease of deployment, making it a noteworthy advancement in the field of AI research.\", 'web_search_needed': 'No'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# User 1 queries\n",
    "user1_id = \"user_123\"\n",
    "queries_user1 = [\n",
    "    \"What is the key innovation in the Transformer architecture?\",\n",
    "    \"How does Mistral's approach differ from GPT-4?\"\n",
    "]\n",
    "\n",
    "# User 2 queries\n",
    "user2_id = \"user_456\"\n",
    "queries_user2 = [\n",
    "    \"Explain the main contributions of the Attention is All You Need paper.\",\n",
    "    \"What safety measures were implemented in InstructGPT?\"\n",
    "]\n",
    "\n",
    "# Ask queries for User 1\n",
    "print(\"=== User 1 ===\")\n",
    "for query in queries_user1:\n",
    "    response = ask_with_memory(user1_id, query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Ask queries for User 2\n",
    "print(\"=== User 2 ===\")\n",
    "for query in queries_user2:\n",
    "    response = ask_with_memory(user2_id, query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User 1 Summary ===\n",
      "The safety measures implemented in InstructGPT include significant enhancements aimed at mitigating potential risks associated with the model's responses. These measures include a reduction of the model's tendency to generate disallowed content by 82% compared to previous versions. Additionally, InstructGPT has shown an increased adherence to policies regarding sensitive requests like medical advice and self-harm by 29%. Furthermore, the model has demonstrated a substantial decrease in producing toxic content, with only 0.73% of generations being classified as toxic, compared to 6.48% in previous versions. To address potential risks related to bias, disinformation, over-reliance, privacy, cybersecurity, and proliferation, InstructGPT has implemented adversarial testing with domain experts and a model-assisted safety pipeline. These safety enhancements aim to ensure that the model's responses align more closely with ethical guidelines and societal norms, thereby reducing the potential for harm in its deployment.\n",
      "=== User 2 Summary ===\n",
      "The safety measures implemented in InstructGPT include significant enhancements aimed at mitigating potential risks associated with the model's responses. These measures include a reduction of the model's tendency to generate disallowed content by 82% compared to previous versions. Additionally, InstructGPT has shown an increased adherence to policies regarding sensitive requests like medical advice and self-harm by 29%. Furthermore, the model has demonstrated a substantial decrease in producing toxic content, with only 0.73% of generations being classified as toxic, compared to 6.48% in previous versions. To address potential risks related to bias, disinformation, over-reliance, privacy, cybersecurity, and proliferation, InstructGPT has implemented adversarial testing with domain experts and a model-assisted safety pipeline. These safety enhancements aim to ensure that the model's responses align more closely with ethical guidelines and societal norms, thereby reducing the potential for harm in its deployment.\n"
     ]
    }
   ],
   "source": [
    "# Ask for summaries for each user\n",
    "summary_query = \"Give a summary of what is discussed so far in previous queries\"\n",
    "\n",
    "print(\"=== User 1 Summary ===\")\n",
    "summary_response_user1 = ask_with_memory(user1_id, summary_query)\n",
    "print(summary_response_user1)\n",
    "\n",
    "print(\"=== User 2 Summary ===\")\n",
    "summary_response_user2 = ask_with_memory(user2_id, summary_query)\n",
    "print(summary_response_user2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Corrective RAG patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.2.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (2.27.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (2.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = getpass('Enter Tavily Search API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool -Tavily API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tv_search = TavilySearchResults(max_results=3, search_depth='advanced',\n",
    "                                max_tokens=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt template for rewriting\n",
    "SYS_PROMPT = \"\"\"Act as a question re-writer and perform the following task:\n",
    "                 - Convert the following input question to a better version that is optimized for web search.\n",
    "                 - When re-writing, look at the input question and try to reason about the underlying semantic intent / meaning.\n",
    "             \"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"human\", \"\"\"Here is the initial question:\n",
    "                     {question}\n",
    "\n",
    "                     Formulate an improved question.\n",
    "                  \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Create rephraser chain\n",
    "question_rewriter = (re_write_prompt\n",
    "                        |\n",
    "                       llm\n",
    "                        |\n",
    "                     StrOutputParser())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Data model for LLM output format\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM for grading\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.5)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt template for grading\n",
    "SYS_PROMPT = \"\"\"You are an expert grader assessing relevance of a retrieved document to a user question.\n",
    "                Follow these instructions for grading:\n",
    "                  - If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "                  - Your grade should be either 'yes' or 'no' to indicate whether the document is relevant to the question or not.\n",
    "             \"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"human\", \"\"\"Retrieved document:\n",
    "                     {document}\n",
    "\n",
    "                     User question:\n",
    "                     {question}\n",
    "                  \"\"\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build grader chain\n",
    "doc_grader = (grade_prompt\n",
    "                  |\n",
    "              structured_llm_grader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Agentic RAG Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional  # Import Optional\n",
    "from typing_extensions import TypedDict\n",
    "from typing import Annotated #Import Annotated\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        user_id: user id\n",
    "        question: question\n",
    "        generation: LLM response generation\n",
    "        web_search_needed: flag of whether to add web search - yes or no\n",
    "        documents: list of context documents\n",
    "    \"\"\"\n",
    "    user_id: str\n",
    "    question: str\n",
    "    documents: Optional[List[str]] # Optional should also have the type of the list\n",
    "    chat_history: Annotated[Optional[list], \"chat_history\"]  # Track conversation history\n",
    "    generation: str\n",
    "    web_search_needed: str\n",
    "    documents: List[str] #This line is duplicate and can be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ====== Node Definitions ======\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"Retrieve documents with user-aware context\"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    user_id = state[\"user_id\"]\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # Ensure the question is a string\n",
    "    if not isinstance(question, str):\n",
    "        raise TypeError(f\"Expected 'question' to be a string, but got {type(question)}: {question}\")\n",
    "    \n",
    "    # Get user-specific memory\n",
    "    memory = get_user_memory(user_id)\n",
    "    \n",
    "    # Retrieve documents (modified to use your existing retriever)\n",
    "    documents = similarity_retriever.invoke(question)  # Pass the plain text question\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"documents\": documents,\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    by using an LLM Grader.\n",
    "\n",
    "    If any document are not relevant to question or documents are empty - Web Search needs to be done\n",
    "    If all documents are relevant to question - Web Search is not needed\n",
    "    Helps filtering out irrelevant documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search_needed = \"No\"\n",
    "    if documents:\n",
    "        for d in documents:\n",
    "            score = doc_grader.invoke(\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            grade = score.binary_score\n",
    "            if grade == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "                web_search_needed = \"Yes\"\n",
    "                continue\n",
    "    else:\n",
    "        print(\"---NO DOCUMENTS RETRIEVED---\")\n",
    "        web_search_needed = \"Yes\"\n",
    "\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search_needed\": web_search_needed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite query\n",
    "\n",
    "# This will be used to rewrite the input query to produce a better question optimized for web search using an LLM\n",
    "\n",
    "def rewrite_query(state):\n",
    "    \"\"\"\n",
    "    Rewrite the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased or re-written question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---REWRITE QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "    rewritten_query = response_state.get(\"rewritten_query\", query)\n",
    "    print(f\"Final Query Used: {rewritten_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Search\n",
    "\n",
    "# This will be used to search the web using the web search tool for the given query and retrieve some information which can be used as the context in RAG\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-written question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = tv_search.invoke(question)\n",
    "    web_results = \"\\n\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "def format_chat_history(chat_history):\n",
    "    \"\"\"Convert chat history into a formatted string with role labels.\"\"\"\n",
    "    if not chat_history:\n",
    "        return \"No chat history yet.\"\n",
    "    \n",
    "    formatted = []\n",
    "    for msg in chat_history:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            formatted.append(f\"User: {msg.content}\")\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            formatted.append(f\"Assistant: {msg.content}\")\n",
    "        elif isinstance(msg, dict):\n",
    "            # Handle dictionary format\n",
    "            role = msg.get(\"role\", \"Unknown\")\n",
    "            content = msg.get(\"content\", \"\")\n",
    "            formatted.append(f\"{role.capitalize()}: {content}\")\n",
    "        else:\n",
    "            formatted.append(str(msg))\n",
    "            \n",
    "    return \"\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_rag_pipeline(user_id):\n",
    "    \"\"\"Get a RAG pipeline for a specific user.\"\"\"\n",
    "    memory = get_user_memory(user_id)\n",
    "    \n",
    "    return (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                # Extract the question string before passing to similarity_retriever\n",
    "                \"context\": (lambda inp: inp[\"question\"]) | similarity_retriever | format_docs_with_sources,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "                \"chat_history\": lambda x: format_chat_history(memory.load_memory_variables({})[\"chat_history\"])\n",
    "            }\n",
    "        )\n",
    "        | prompt_template_with_history  # Ensure this is defined; if not, use prompt_template\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_answer(state: GraphState):\n",
    "    \"\"\"Use your existing multi-user RAG pipeline for generation\"\"\"\n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    user_id = state[\"user_id\"]\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Get user-specific RAG pipeline\n",
    "    rag_pipeline = get_rag_pipeline(user_id)\n",
    "    \n",
    "    # Format input for your existing pipeline\n",
    "    inputs = {\n",
    "        \"question\": question,  # Pass the plain text question\n",
    "        \"context\": format_docs_with_sources(documents),\n",
    "        # \"chat_history\": state[\"chat_history\"]\n",
    "        \"chat_history\": format_chat_history(state[\"chat_history\"])\n",
    "    }\n",
    "    \n",
    "    # Generate response using your existing flow\n",
    "    response = rag_pipeline.invoke(inputs)\n",
    "    \n",
    "    # Update memory\n",
    "    memory = get_user_memory(user_id)\n",
    "    memory.save_context({\"input\": question}, {\"output\": response})\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"generation\": response,\n",
    "        \"chat_history\": memory.load_memory_variables({})[\"chat_history\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Decide to Generate\n",
    "\n",
    "# This will be used as a conditional function which will check the web_search_needed flag and\n",
    "# decide if a web search is needed or a response should be generated and return the function name to be called\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search_needed = state[\"web_search_needed\"]\n",
    "\n",
    "    if web_search_needed == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\")\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE RESPONSE---\")\n",
    "        return \"generate_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /opt/anaconda3/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from langgraph) (0.2.43)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.147)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/lib/python3.12/site-packages (from langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.3,>=0.2->langgraph) (2.27.2)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2->langgraph) (2.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "agentic_rag = StateGraph(GraphState)\n",
    "\n",
    "#Modified node list\n",
    "agentic_rag.add_node(\"retrieve\", retrieve)\n",
    "agentic_rag.add_node(\"grade_documents\", grade_documents)\n",
    "agentic_rag.add_node(\"rewrite_query\", rewrite_query)\n",
    "agentic_rag.add_node(\"web_search\", web_search)\n",
    "agentic_rag.add_node(\"generate_answer\", generate_answer)  # Now uses your RAG pipeline\n",
    "\n",
    "# Edge setup remains similar but now handles user-aware state\n",
    "agentic_rag.set_entry_point(\"retrieve\")\n",
    "agentic_rag.add_edge(\"retrieve\", \"grade_documents\")\n",
    "agentic_rag.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\"rewrite_query\": \"rewrite_query\", \"generate_answer\": \"generate_answer\"},\n",
    ")\n",
    "agentic_rag.add_edge(\"rewrite_query\", \"web_search\")\n",
    "agentic_rag.add_edge(\"web_search\", \"generate_answer\")\n",
    "agentic_rag.add_edge(\"generate_answer\", END)\n",
    "\n",
    "# Compile graph\n",
    "agentic_rag = agentic_rag.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCAJ2ANIDASIAAhEBAxEB/8QAHQABAAIDAQEBAQAAAAAAAAAAAAYHBAUIAwIBCf/EAFwQAAEDAwEEAgsJDAUJBgcAAAEAAgMEBQYRBxITITGUCBQVFhciQVFW0tMyNFVhY3WTs9EjJTY3U1RxdIGVstQYNUJSoSQzYnJzkbTBwiZDRJKisScoOEZklvD/xAAbAQEBAAMBAQEAAAAAAAAAAAAAAQIDBAUGB//EADkRAQABAgIGBwcCBgMBAAAAAAABAxECEhQhMVFSkQQzQXGhsdETYWKBksHSBeEVIiMysvBCQ1Pi/9oADAMBAAIRAxEAPwD+qaIiAiIgIiICIiAiIgLwqq2noWB9TURU7D0OleGj/FaOqra3Iq2oobXO6hoqd5iqrk1gc9z9OccGuo1GvjPIIafFALt4x/dLgGP0zzI61U9ZUnQuqq5vbEziOgl8mrvP5fKt+TDh6yflH3/2VtvZnfVZfhig6yz7U76rL8MUHWWfanerZfgeg6sz7E71bL8D0HVmfYr/AEff4LqO+qy/DFB1ln2p31WX4YoOss+1O9Wy/A9B1Zn2J3q2X4HoOrM+xP6Pv8DUd9Vl+GKDrLPtTvqsvwxQdZZ9qd6tl+B6DqzPsTvVsvwPQdWZ9if0ff4Go76rL8MUHWWfas2kr6avaXU1RFUNHSYnhwH+5YXerZfgeg6sz7Fh1ez/AByskEps9LT1IOraqkZ2vO0/6Mse68eToPkS1Ge2Y5T94TUkCKNQVVbitRDTXGpluVsneI4bhK1okgeeTWTFoAIJ0DX6DmQHak7xkq148GX3wCIi1oIiICIiAiIgIiICIiAiIgLS5nd5rFi1zrabd7bjhIp98eLxXeLHr8W8W6rdKN7RoJJ8KujomOkfTsbVhjRq53Ce2XQDyk7mgW6jETVwxi2XhY2ttY7RDYbRSW+n14VPGGBzjq5x8rifKSdSSeZJJWcviCZlTDHLE4PjkaHNcOggjUFfa14pmZmcW1BQ/aDtcxPZabc3JboaKa4GQUtPDSzVM024AZHCOFj3brQQXO00Go1I1UwVFdk3RxxusF3t9uzFmX2+KrdZr3iNtNb2rI5rNYKmPRzXRSkN5Obu/cz4zOROI21Z2SVipNslqwftWumguNnjuUNyp7fVzNc+WWNkTNGQkNYWv3jK5wa06BxaVvRt+wLv5biDr9wr86qNCyKajnjhfUDXWFs7oxE6TkfFDyfJoqyhueW4ztYwLNssxS61U1zwkWi5tx+hfWCiuRnhmeyRsepYz3YDubQW6a+VVvndBmeR3QVN9s2f3XJrRnFLcO16OCfuJTWqCva6J9OxhEdQ7ghp5B8ocXagAFB0tLt7wiPKLjjjLpU1d7ts5pqyjorXV1D6d4jEnj8OJwa0tI0cTuuILQSQQNfsA2727bviXdWmoau21kb5BPSz0lQyJjeNKyPcmkiYyUlsertwndJ0OhWJsRx6ttGc7YqytttRRC45QJqaeeB0YqYRQ0zQ9jiPHYHCQajUahw6dVq+xWqLhjeEDBLzj16tN3sVRXcapq6F7KKoa+slex0E+m5KHNkafFOo569CC8EREGLdLbT3i3VNDVx8WmqY3RSM101aRoefk/StXg9ynumL0clW8S1kJkpKiQf25YZHRSO/a5jj+1b0kNBJOgHSSo1s5YTikNSQ5orqmpr2Bzd07k9RJKzl5PFe1dEa6M98eU/svYkyIi50EREBERAREQEREBERAREQEREEUt9RHggZbKwtgsYO7QVZ14dO3yQSnoYB0McdAQA06OA3/LKtjuCZ3cxcsiw+x324CMRCquFBFPJuDUhu85pOg1PL41LpI2TRvjkY18bwWua4agg9IIUa8H1upj97Kq4WVnL7jb6t7IW6dAbEdWNHxNaP8AuiZwVNeObTzv8A782WqUePY2bJzprs3xY6dH3og9VSnENn+M7P6WemxmwW3H6eoeJJYrbSsgbI4DQOcGganTlqsbvJqPSq/fTQ+yTvJqPSq/fTQ+yT2dPj8JS0b0oRRfvJqPSq/fTQ+yUTnt91j2rUWPDKbx3Olss9e4mWHicVk8LG6Hh+53Xu15dOnNPZ0+PwktG9ai0eXYNjufW6KgyWx2+/0MUonjp7jTMnjbIAWh4a4EA6OcNfMSsPvJqPSq/fTQ+yTvJqPSq/fTQ+yT2dPj8JLRvaBvY37KWNeG7OMXaHjdcBaYBvDUHQ+L5wD+xbXFtjOB4RdW3PHsNsdkuLWOjFXQW+KGUNPSN5rQdCsrvJqPSq/fTQ+yX6dn1DVO++Vbc7vHrrwayseYT/AK0bd1rh8TgQmSnG3Hyj1sWje87nXMzYT2i2StltrtYrhXxO8QN6HQxuHunn3LiD4g1/taBSmONkMbWMaGMaA1rWjQADoAC+YKeKlgjhhjZDDG0NZHG0Na0DoAA6AvRYY8cTEYcOqIJkREWpBERAREQEREBERAREQEREBERAREQEREBV7Vaf0gLZ0697FX5OXvun+P8A5KwlXtU0/wBIG2O0OgxirGu7y990/l/5ILCREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQFXtVp/SBtnud7vXq/Pr77p/wBmisJV9VA+H+2HTxe9iq58/wA7p/2ILBREQEREBERAREQEREBERAREQEREBERAREQEREBFDZsvutzfJJYqCjloWucxlXXVD4+MQdCWMaw+JrqA4ka6agFpDj5d3cw/MLH1ub2a646LU7bR84WybooR3dzD8wsfW5vZp3dzD8wsfW5vZq6Lj3xzgsm6/n/duz2u9D2RUdpfspndktKyXGRa2XlpMk76iMhwf2v0asGnLmHArsnu7mH5hY+tzezVQVuwCau7Iik2vSW+zd2YKLgdqdsSmJ84G42oJ4fuhGS3T4mnpHNouPfHOCzpZFCO7uYfmFj63N7NO7uYfmFj63N7NNFx745wWTdFCO7uYfmFj63N7NO7uYfmFj63N7NNFx745wWTdFC25hebSO2L3bqJlub/AJ6poKl8joG+V7mOjGrR5SDqBz0OhUzBBAIOoK01KWKnbMWs/URFpQREQEREBERAREQEREBERAREQVzs3O9s8xdx01da6Vx0GnMxNJUjUb2a/i5xb5qpfqWqSL2a/W4++fNZ2yIiLSgiwbJfLfklrguVqrYLjb5wTDVU0gkjkAJBLXDkRqDzCx73lVrx2ttFJcKk09RdqrtKiYInv4s246Td1aCG+Kxx1doOXTqQoNsiIqCIiDU5eAcTvQIBBop+RGv/AHblKrC4usduJOpNNGST/qhRXLvwUvX6lN9W5SnH/wCobb+rRfwBY1+pjvnyhexsERF5yCIiAiIgIiICIiAiIgIiICIiCuNmv4ucW+aqX6lqkijezX8XOLfNVL9S1SRezX63H3z5rO2XI9qv+RYr2Pmd7T++O+XjIbfV3iC3wVlwlkpKWMV0kLSYNd2ThgF4Lw4gANGjQApfs72ebSafJrfLXXSoGKV9FUQ3V0uZ1F1mqA+I8KamJpYe13h+h3onNGjuQ1AV1WPA7BjuOVVgobbE2z1UlRLPRzOdMyUzvdJNvb5dqHOe8lvRz0AA5LSYJsRwvZpcn1+OWd1BVOgNM1z6yedsUJcHGONsj3CNmrWndYAPFHmXLlnUjmjE7ZW4h2D1oumPZFfLZd7nU2uLtoXOeUUpN0ZE4Qsc4tjBD3BzWgB3Q4FWxm2NTbPs92Rx23JMmnjrsgnpqyOvvdTUR1THUU7yJGOfuuAdE0gaaN57oGqmtN2PmA0dLc6WCxOho7lUxVlRSMrqkQcWOcTscyPibsekrQ7RgaD0EEclLL5iNpyS4WWuuNJ2xVWaqNbQycR7eDMY3xl2jSA7xJHjR2o566agJGHUOVNm52v7V7Bbs+tNd2tcKy4um1qMrmbRQxMqSx9K+2ikMY0Y1zNd/f3vG39eSkdZf8khzKv2Mi93UXWtySO50117bkFTHj79aqXdm13xuyRSUoOvIPYFb8WwLAqfMHZPDYGwXd1WK9zoaqdkDqnp4xgDxEZNee/ua689dVLnYza35LHkJoojemUjqBtbp90EDnh5j/RvNB/YkYZHK1Mdq+1+5Zvecer5KGutl+rbTbnd9UtHT0Ha8m7G2agbSPjm1AD3cR5Lg/kWDTTrmDicCPjbom3Rv7nRvac9PiUCvewPAshyuTJK2wNdd5pI5Z5YaqeGOofHpuOliY8RyuGg0L2k8grAVwxMbRqcu/BS9fqU31blKcf/AKhtv6tF/AFFsu/BS9fqU31blKcf/qG2/q0X8AVr9THfPlC9jYIiLzkEREBERAREQEREBERAREQEREFcbNfxc4t81Uv1LVJFo4LZesSp226mtEt7t8A3aWalniZIIuW6x7ZXtGrRy1BIIAPInQfvda/ehl161Re3XtY7VMc48OKLTN9serKYvN27RaTutfvQy69aovbp3Wv3oZdetUXt1hk+KPqj1LN2i0nda/ehl161Re3Wufm9ezIorE7FLqLpLSvrWQcek5wte1jnb3G05Oe0aa68+hMnxR9UepZLEWk7rX70MuvWqL26d1r96GXXrVF7dMnxR9UepZu0Wk7rX70MuvWqL26d1r96GXXrVF7dMnxR9UepZ65d+Cl6/Upvq3KU4/8A1Dbf1aL+AKHz0V9ymmmtstmmsdJUsMVRVVdTE57YyCHCNsT36vI5AktA1156bpnsUTIImRxtDWMAa1o8gHQFz9ImIwRgvebzOqb+ROyz7REXAxEREBERAREQEREBERAREQEREBERAREQFX9UP/j7bTp/9s1XPT/8qn8un/P9nmsBV5Vf/UHbPFH4L1fjc9ffdMgsNERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAVeVWn9IK2dGvevV+fX33TfsVhqvap3/zA2xup54xVnTXl77p/IgsJERAREQEREBERAREQEREBERAREQEREBFGqjaViVJO+GbJrRHKw7r2Gtj1afMefIrz8KWHelNo67H9q6NHrT/AMJ5Stp3JSii3hSw70ptHXY/tTwpYd6U2jrsf2po9bgnlK5Z3JSii3hSw70ptHXY/tTwpYd6U2jrsf2po9bgnlJlnclKKLeFLDvSm0ddj+1PClh3pTaOux/amj1uCeUmWdyUqj6rbLgA230FZ38432mzHamJ0/din4YeamAhpPE010BOnmBVj+FLDvSm0ddj+1fzvv3Yu4tWdmjDNDc7WNmdVJ3enmbUx8BhDtX0moIALpOQaOe47XyJo9bgnlJlnc/pyii3hSw70ptHXY/tTwpYd6U2jrsf2po9bgnlJlnclKKLeFLDvSm0ddj+1PClh3pTaOux/amj1uCeUmWdyUoot4UsO9KbR12P7U8KWHelNo67H9qaPW4J5SZZ3JSii3hSw70ptHXY/tTwpYd6U2jrsf2po9bgnlJlnclKKNQ7TMRqJAyPJrS95IAArY/KdB5fOQP2qSrXjp46f98THekxMbRERa0EREBERAUV2i1D2WiipWvcyOuroKWYsJBMbjq5uoII1DdDp5CVKlENpHvWw/O9P/1Lp6NF62FY2smCCOlhZDDGyKJgDWxsaGtaPMAOhfaIupBERAREQEREBERAREQEREBERAREQfMkbZWOY9oexw0LXDUELG2dScKmvVtYT2rbLgaanZ5I4zDFKGD/AEWmUgDyAADQALLWDs99/wCYfPA/4OlTFrpY4n3ecLGyUxREXmIIiICIiAohtI962H53p/8AqUvUQ2ke9bD870//AFLq6N12FY2sxRPajtCg2Y4fPe5aKa5zmeCjpKCncGvqaiaVsUUYc7k3Vzxq49A1PPoUsVW9k5b6e57EcjhqZqSlYBA9tVWCfcgc2eMiQOga+Rjm6ateGuDXAOcC0ELonYiI3fsm73its2hTZBg0dFccQhtsslvpbv2wasVcxY0xycFo0AHlGpcCNANHGV5RtSzDEcPberlhtnoJXVRjMVwymGlggg3AWyTTviAa8uJbw2B41Gu8QdVRGz62Ve2XDdoWLWptvuV4uHcysnzRl3qq+lrXRVDSKd80lNGQ+OOIkMYwtHE56Ekm+dr+za+5XluF5JYo7LcZ8fdVa2rIHSMppDMxjRM1zGPIkj3Du6tPKR3NvStcTMxcaeh7JujvmzvHL5ZbBNdciv8AcZbRQ4/DWRneqoi/ja1A1ZwWNic/igEFu6dOeihVr2/3LAL/ALV7tnFK+31EN0tNut9hkvLJKWGeakaQ1lRJuRxRu0MrnkN0G8SCRodhaex6zbH7Haq6humPnMLDk9wvlA9zJmUFTBWNcJ4JGgF8X+deGlpfoGNPMkgflx7HrNsrrMqv92uNgtuT1N7td/tAouNUUkU9HT8Hhzh7Wucx7S9pLeejtdBpuqfzCa7HOyDodqmR3XHpILXT3igpY67WyXuG7UksLnFmomjDd17XDQsc0HRzSNQVYWa5hbdn+JXfJLxK6G2WumfVTuYNXbrRro0eVx6APKSFEbLkOQYTaqu55/Q2mlEkscFNBh9BW3FzeTi4ybsO+QSBpowBumhJJC1mZ3fGtv2G33AoH36hlvNFJCyqq8duFLHC4Dea8vmhYzk4A7pcCdNB0rO+r3iDXDa/lrNq+z+vyuwVOB42bTerlNTi7ipFRFHBFJrURMa0NfGOe6d/TfOh11Uvxbb/AHS43LE3ZDhUuM2DLSW2W5yXFk73PMTpo2VEQaOC58bXOADn6EaHQrR12x/aJtIyPHptoE+MC2UFputpq3WOaoM1SKyBkRkDZIw1p0bqW68tel3QPexbGs6vFVgdszW42CfGsMeJ6d9q43bNzmjgfBC+Zr2hsOjHuc4Nc/V3lAWMZhI9n22LIdpdTRXS0YO5uC1ssjaa/VN0jjnkjaXATil3NeG5zfF8feIIO6AvC09kB3U2ZbOMu7g8LvxuVHbu0+3Ne1OO5w39/h/dN3d6NG669IXhsnwLaPsvpLRiDavGrlhFre6GC4SmobcnUmriyN0Qbw99urW7+/oQ33OpUTs2wfaDbcd2e4i+txt2N4dfqa4RVjZajtuspoZHlrXM4e5G8NfzAc4OI6W+W3xWGwq+yev1PbDeYdnjqqxDIZca47LzGJ5KkVTqaNzInRgcNzwwEue0tLjycG7x21Z2SsOJ2nNn5lYm2G74u+jbJR09xZUQ1Qq9RTFk72xhu85rg4vDQzdJJ0Gqw27CL+NnUVg7ctvbjM375S/iycPtbun23ua7mvE4fLTTTe5b2nNfW0DsernnOR7Qbky6Utufd2WSos1QGulfTVdA+aQOmjIALHGRo0DiSC7oIGs/mGpoOzDopbflQqbXaKi62aw1F/hgsWSQXOnqYodA+N00bNYpNXM5FhBBJBOhUysm2a/V2UU1huWGss9dd7RUXaxcS7NlbV8Lc3oZy2P7g/7rGTu8VoBOhOmhwr5gm0PPNmOcY9kEGH22uu1okt9AbO+ocwSvY9rnyyPjBDCSzRrWOI0PNy39Zs3udRtJ2d5C2ekFFjtpr6CrjL3cR8k7aYMMY3dC0cF2upB5jQHnpdYrDZ12QmWWTsa6HOsystPcp5TTw0tVFcms7cfNUGLfqNYWMpWNJbqRvjTzdBvnC7xd77YIay+WeGx173O1paeubWRluvivbK1rd4OHMeKD8SqnBdm20XAdl1RhEMWG3mioAae2SXJ1QWVlM6ZznsqoxGQw8N274peNeZBHJSrYJs1uey3Da213OaiD6m51FfDb7W6R1HbYpCC2mgLwHbjSCeho1edAAmG+q4slYOz33/mHzwP+DpVnLB2e+/8AMPngf8HSrbPVY+6POFjtTFEReWgiIgIiICiG0j3rYfnen/6lL1FdolM+S0UVU1jpGUNdBVShjS5wja7RztACToHF2g8gK6ejTathWNr3RedPUxVcDJoJWTQvG8ySNwc1w84I6V6Lq2IIiICIiAiIgIiICIiAiIgIiICIiAsHZ77/AMw+eB/wdKsyWVkEbpJHtjY0alzjoB+1Y+zuPi015uTGkU1zuBqqd5/7yMQxRB45e5dwiQfKCCORCYtVLHM+7zhY2SlqIi8xBERAREQEREEbqtmuJVtQ+eoxizzTSHefI+giLnHzk7vMry8FeGeidk/d8XqqUot8dIrRqjHPOVvKLeCvDPROyfu+L1U8FeGeidk/d8XqqUorpFbjnnJed6LeCvDPROyfu+L1Vqr3gmHUZFDRYlYZbzUwyuo4ZbaOEXtaSDK9kbuGzXQbx8+g1JAMov17dbYxTUQpaq+VEb3UVvqKoQcct3Q4k6OcGN32lzmtcQCNASQDk2u1ttjaj7vUVMlRKZ5H1Ezn6OIA0YCSGNAA0a3QdJ6SSWkVuOecl53oxb9jmGULZS7GbTUTTv4sr5KNhbvbrW+I0ghjdGjxW6DXU8ySTl+CvDPROyfu+L1VKUTSK3HPOS870W8FeGeidk/d8Xqp4K8M9E7J+74vVUpRNIrcc85LzvRbwV4Z6J2T93xeqngrwz0Tsn7vi9VSlE0itxzzkvO9D6zY/hVa2IOxe1R8ORsoMNJGwktOuh0HNp6CDyIK1tiwTEZ3NttyxSwNvkEDJKlsFqDIJNSRvxOezRzdRqWhzizeaHHmCbCWFdLY26QxMNRU0zopWzMkppnRneadQHaHRzT5Wu1B8oTSK3HPOS872j8FeGeidk/d8Xqp4K8M9E7J+74vVW0x68S3Sk3K6KnorvAA2toIKptQIHno8YAEtcPGaXNaS0glrTqBtU0itxzzkvO9FvBXhnonZP3fF6qeCvDPROyfu+L1VKUTSK3HPOS870ap9mmI0kokhxezxSDmHMoIgRz1/u+cKSoi146mOp/fMz3l5kREWtBERAREQEREBERAWJdbiy0WusrpIqieOlhfM6KkhdNM8NaSWsjaC57jpoGtBJOgCy1oLrRS3jJbbTzUVSLdQjt8VjKoRxPqBqxkTowd54Ac5/PxQWsPM6boZNjoKmLtisrpnz1dVIZGMlhiY6kiOm7Tgs11DdNSS52rnOIIbutbtkRAREQEREBERAREQRzKXMsDhkgmio6eiYXXR4oTPLUUjGSEMaWDfBY9++NA4acQbur95shY9srGvY4PY4atc06gjzhfS0GISvgp661Sz3GsntlS6B1XcYd10zXASsLXgaSNa2RrN8dLmOB8YFBv0REBERAREQEREBERAREQEREBRvFbfw7xk1xltDbbVVdcGGcVfHNXFHExkchGpEXQ4bg06N483FYm1y75PYNmuQ3PDKSir8moqV1RR0twje+GYsIc5haxzXElgcG6EeMR+hcr9gp2RW0TblluQ09Zj2N2XFaJ8tdcZ6CmqhNLWzvLg1rpah4aS7ecRu6AN0AGoQdroiICIiAiIgIiICIiAo64mj2gs5XmYXG2EEjx7bTmnlGmv5OeTto/67YOf+bCkSjuSgxX/FqgMu0v+WyQObb3f5O0Op5Tv1TfLGCwAHyPdH5CUEiREQEREBERB51NRHSU8s8rt2KJpe53mAGpKgUE9+yanhuIvlTY4Khglho6KCBxYwjVu+6WN5LtOnQADo56amW5V+DF4/U5v4Co9jX4OWr9Ui/gC9Do8RhwTjtEze2uL+bLZF2N3HvvppeOrUP8unce++ml46tQ/wAut2i3+0+GPpw+iXaTuPffTS8dWof5dO4999NLx1ah/l1u0T2nwx9OH0LtJ3HvvppeOrUP8unce++ml46tQ/y63aJ7T4Y+nD6F2k7j3300vHVqH+XUYwTY5S7MqS402MX25WiC41slwqmQwUZEk79N53jQHQcho0aNHkAVhIntPhj6cPoXaTuPffTS8dWof5dO4999NLx1ah/l1u0T2nwx9OH0LtJ3HvvppeOrUP8ALp3HvvppeOrUP8ut2ie0+GPpw+hdpO4999NLx1ah/l07j3300vHVqH+XW7RPafDH04fQu0rbTfWnXvyuzuR0D6ai0/wpwf8AFbrFb3V1dVXWu4lkldRNjkFREwsbPC/eDHFv9lwLHtcASOQcNN7daWsxz8Y98+aaH66rWOO2OnimYjVHZER2xHYu1NkRF5TEUdzSMuhs8gZdpDFdaZ27aHaOOr9zWYeWEb2rx5hr5FIlHc7i4lmpCIbnUFt0t7ty0v3ZR/lkPjO88TfdSjyxtkQSJERAREQEREGryr8GLx+pzfwFR7GvwctX6pF/AFIcq/Bi8fqc38BUexr8HLV+qRfwBejR6me/7L2NkiLiOw4zDhXYTjM7BB2rlFXT9r1t/DXyVUFA+4Bs4aWuD2xtiaTusLdA0uBB1ckzZHbiLk3Hux4oMghyC12rMMQNvu2PT001oxSnmiZO6QtNNWSB9XON6ORurZA0E6uBJ8kQZtf2idw/C6KOtMVRQDCW2fc5i4CLxazTo07oF8GvTukfoWOa22B3Cohetp9rxi2ZPc73SXKz2uwSMjmraqkPDqQ5rCHwbupkbrIGE6DRwI8mqojDdheNU23OmxW8UMV5oLBgNribT1OroJpxV1W9O9h5OfvB7gSORe4jmojtHw+zU+y3slqOK1UwoqLIqauihbEC2J/a1I+SQDyHx5CT/pOScU2HaiLk3bBZbJNmGzfCbDU4tZNnNVS180FPWQOmtFTXB0b2xPZDPC0u3HyPaHOIJc47pOhFw9jzh8mGYfcKRmT23JrbNcppqLuQ14pKFmjWPpot+aZ262Rkh0LzulxHLTRWJvNhaKKj+yiqYHU+AWu9Vr7fhd0yKKkvszZnQsfDwZXRRSyAjdifK1gcdR5BrzVIZtaMcxqDbjR4uKWDH7fNh8ojo5+JBTMbXGSXTmQxo8ZxA0A1J8pScVh2+i5K7IPM6yk2kZ1W4dcG1F7t+zWQ8S3yCSSmDq5hc8bp5OEZc8eXkD5l67O9mVDabnT33Hs1w91K+x1k1TbcZpqiKa7U74dGyz8Wtm3iyR0buIW72pIJ8ZTNrsOsEXJGJbHLdS9iZieW45a4351brTb8ip69+r6iolp2iYQF51O46MyQtb0BrwNNArO7Hq5w7TLvmG1OJrzRX+pjt1nMrdHC30jSwEa9G/O6ocR/qqxiuLpWsxz8Y98+aaH66rWzWsxz8Y98+aaH66rW3/rqd33hY7U2REXlIKOZ9GJbBCDDdJ9Llb3blndpPyrITqfkhprL54hIpGo5nzQ/H4gWXd/3xoDpYzpUcqyE6n5IdMvyXEQSNERAREQEREGryr8GLx+pzfwFR7GvwctX6pF/AFIsoaXYzdmgak0kwAH+oVHcZION2oggg0kWhB6fEC9Gj1M9/wBl7GyWDbbDbLNaY7Xb7dSUNsjaWMoqaBscLWkkkBjQAASTqNPKVnIskaPGsExrCzUHH8etViNQd6buZRRU/FPnduNGv7VsO41v7UFL2jTdrCbtng8Fu5xeJxeJu6ab3E8fe6d7n081mIoMNlmt8d3luraGmbdJYW00laIWiZ8TXFzYy/TeLQXOIbroC4nyrzjx20xd09y2UbO6bt+v3adg7bduBmsvLxzuAN8bXkAOgLYIgjQ2Y4cMcOP96dj7gGTjG1dzYe1S/wDvcLd3dfj01WHddn0/adBRYxkNZgtupGOYKKxUNDwXanX3M1PJu6c/c6DmddVMUS0CIWnAJ+0LjQZRkFXnVurWtY6jvtDRcFoBJPiwwRh2vLXf3vcjTTmtThuxGx4VlWY19DS2+Gy5FTUVKbFT2+OGmgbAyVrhug7rw/ikkbo008uqsVEtA0Fg2f4vikrZbJjdos8jYjTh9voIoCIy7eLNWNHilwBI6Nea+bJs6xTGZqyaz4xZrVLWgtqpKG3xQunB6Q8taN4H49VIUS0CM5HidXNhncDE7jT4duRsp6ealoI5WU0I0BZHCSGN8XkOWjeXI6aLKwbDbbs8w6z41aGOjttqpWUsAeQXFrRpvOI01cTqSfKSVvESwLWY5+Me+fNND9dVrZrW44Ndot8PSBaqEHn0fdqpZ/8AXU7vvCx2pqiIvKQUdz1u9YIhu3h/3xoDpYzpUe/IeZ+S/K/JcRSJR3PW71giG7eH/fGgOljOlR78h5n5L8r8lxEEiREQEREBERB+OaHtLXAOaRoQegqFuw692r7hZbrRMtzeUVPcKV8r4W/3GyNkbq0dABGoHlKmqLdTq4qV8vqt7IT3BzD4TsfUZvbJ3BzD4TsfUZvbKbIt2lVN0coLoT3BzD4TsfUZvbJ3BzD4TsfUZvbKbImlVN0coLoT3BzD4TsfUZvbJ3BzD4TsfUZvbKbImlVN0coLoT3BzD4TsfUZvbKP4bXZdl9Lc5m1Nlpe0bnV20tdSTO3zBK6Pf8A86NA7d108mqtZV7sXdvWrKNBp/2nuw8n50/zJpVTdHKC7M7g5h8J2PqM3tk7g5h8J2PqM3tlNkTSqm6OUF0J7g5h8J2PqM3tk7g5h8J2PqM3tlNkTSqm6OUF0J7g5h8J2PqM3tk7g5h8J2PqM3tlNkTSqm6OUF0JFgy8nQ3OyAecUEx0+PTjc/0Lf49jzbIyollndWV9U4PqKlzd3e05Na1v9ljRyDfjJJJJJ26LXjr48cZZ2e6IguIiLnQUdz1u9YIhu3h/3xoDpYzpUe/IeZ+S/K/JcRSJR3PW71giG7eH/fGgOljOlR78h5n5L8r8lxEEiREQEREBERAREQEREBERAREQFX2xg62rKOZP/ae69Lt7/wAU/wD/ALTyKwVXmxX+qcp6Pwou3QB+dPQWGiIgIiICIiAiIgIiICjuet3rBEN28P8AvjQHSxnSo9+Q8z8l+V+S4ikSjuet3rBEN28P++NAdLGdKj35DzPyX5X5LiIJEiIgIiICIiAiIgIiICIiAiIgKvdjH9VZRzB/7T3boAH/AIp/mUiz3PbHsxxK4ZPkta632SgDHVNU2CScxhz2sB3I2ucRvOb0A6dJ0AJVHdjL2SmzjPLxesYsGQSXG9116ulzgpm26qYDTunfI2Rz3RhjQWke6cDqQNNTog6RREQEREBERAREQEREBR3PW71giG7eH/fGgOljOlR78h5n5L8r8lxFIlHc9bvWCIbt4f8AfGgOljOlR78h5n5L8r8lxEEiREQEREBERAREQRq453S0dZPS0tBX3eWB25MaGJrmRv5atLnOa0uGo1AJ08qxPCJJ6LX36On9stVs6dxMDsEp93NRRTPPne9oc4/tJJ/apEvVxUqVPFOCcN7e+WU2jUwvCJJ6LX36On9snhEk9Fr79HT+2Waixy0uDxlLxuYXhEk9Fr79HT+2TwiSei19+jp/bLNRMtLg8ZLxuR/Jsho8vx252O64ffKq23GnkpaiF0dP40b2lrh/nuR0PT5Fz12G+weTsan5XXXOw3S53i5VJp6Wpp44Tw6Fp1YDrKNHvPNw5jxG6ErqVEy0uDxkvG5heEST0Wvv0dP7ZPCJJ6LX36On9ss1Ey0uDxkvG5heEST0Wvv0dP7ZPCJJ6LX36On9ss1Ey0uDxkvG5heEST0Wvv0dP7ZBtEcD4+M31jfK7hQu0/YJST+wLNRMtLg8ZLxube1XalvdDHV0cvFgfqNS0sc0g6FrmuALXA6gtIBBGhAWYobhbi3KMriHKPi00u75N4whpP8AuY3/AHKZLirYIp48se7xi5MWERFpQUdz1u9YIhu3h/3xoDpYzpUe/IeZ+S/K/JcRSJR3PW71giG7eH/fGgOljOlR78h5n5L8r8lxEEiREQEREBERAREQVzs2/F7jPzbT/VtUjUc2bfi9xn5tp/q2qRO10OhAPkJGq9mv1uPvnzWdsv1FznaeyoqoYcDor1bKdt6rbpVW3J20jXiK1cGo7TEnNxLWvqZabQuJ8V7vKNR4Xnsn7zDbqN1DQ0XFyG+3KjsNT3PrKyNtvotGSVMsNOHyyudIDuhgYN17SSNCTzZoR0mi5GzXaZle0LGsbhqbTDR3y2Z7aIqK41FtraGgr98PcyQRVDGzNDXate3n7nk7RwV0bNtoeTVWf3/Bc1pbWy/W+jgulLXWUSNpqykle+MHhyFzo3tfG5pBcQddQkYryLRRQ7bBl93wLZpf7/YrSb3daGASQ0W69wd4zQ5xazxi1jS55DeZDSBzVVXbsirrYdnVhuDbhjeVXrIro6gtlbj1LWT0bI2xcSSSWnj4k5ezdeDG3nqW6lvjbtnFEDoZFz7YNveT1WI5pPdo7RZqqxtpZaXIrta7jbLXUtlcWuaYahom4jN0jca528XxgEanTR0/ZRZIzZhtRuUtHa6zIsOjpZ4J46CspKSsjnGrCaeoLZmEbrx7rQ8iDoVM0Dp5FSt+2g7R7NesdxJkGNT5hkbqirgfwpxRWuigZGZDN90L6iTfkDGlvDDtdSG6KNbSRnzdpOxdkr8cfmBqLy0TsZO23tZ2r7sxlxkJ3Oe5ve65b2nNJxDo9FX2xjPrtnNnvsN/paOmvtgvNRZa11uL+1pnxhjxJGHkua1zJWHQkkHXmrBWUTca/DPwuyr9NL9WVM1DMM/C7Kv00v1ZUzWjpXW/LD/jDLFtERFyMRR3PW71giG7eH/fGgOljOlR78h5n5L8r8lxFIlHs7jMlhjAbd3ffChOlkdu1HKrhPP5Ll91Hli4gQSFERAREQEREBERBXOzb8XuM/NtP9W1SNRzZt+L3Gfm2n+rapGvZr9bj7581nbKqZ+xzxqpuW02sk3jJncLIarRunaobDuax8+Ti/WUnl42nmBXjdOx8gZieBUOOXyXHL7hMQitV3ZTNnaQYhFM2WFxAe2UDVw3gddCDyVuIufLCKnyfY9kuZYjaKC7Z3xr9bb3De4btFaI2MY+IHhxNg39NwOIJ3nOJ5jXmCPmx4Xd9mlyvGY3d9z2l5bdhBQyG00lNRimpY99zGRRSzta1gc9xcTI5xLhy0CtpEtArmfKsozSlqLRb8ayTA66ZmsN+uUFuqYKdzSHaOiZVPLt4At00/tdI6VDYuxdkNvra6XLpYs3mvzcigv1Dbo4IaeqEIh0bS7zmmN0YIeC7V5OpKvhEyxO0VRftj2RZjhRtmQ5uLhfae6014t9zhtEcMFJLA5jo29r754jN5pJDn6neOhGg0j127Gq75FbdoUd2zjt2uzShpKWrqBaWxsppKdztx0TGy+43Hbu45zjqN7f8ivhEywK/wBpuy2pza7Y/f7JfpMYymxOmFHcRStqonRTNDZYZYXFu+x2608nAgtBBWJTbKr1V5LhF/v+WC9XLG5q+V72W1tO2pFTFwwxrWvPDDB598nynyqy0S0CIbPdn3eHV5fP2/293wX2a9bvB4fA34oY+F7o72nB13uXuujlzl6IrsGvwz8Lsq/TS/VlTNQzDPwuyr9NL9WVM1o6V1vyw/4wyxbRERcjEUcz4gWCHedeGjulb+di98e/IdAfkfyvyXFUjUczx+7ZKb7pdot66W5u9ZRrPzrYeTvkT0Sn8kZEEjREQEREBERAREQVnaq2LBbVS2W6sqIDQsFPDUNppJIp4m6Bj2va0jUt01aeYIPSNHHI7/7H+dS9Vm9RWIi9Cek4MU5seGbz7/2lleJV33/2P86l6rN6id/9j/OpeqzeorERNIpcE84/E1K77/7H+dS9Vm9RO/8Asf51L1Wb1FYiJpFLgnnH4mpXff8A2P8AOpeqzeovKn2k45VteYLgZgx7o3GOnlduuB0c06N5EHkQrJVebFQG2vKmjXUZRdtdfjqnn/mmkUuCecfian53/wBj/Opeqzeonf8A2P8AOpeqzeorERNIpcE84/E1K77/AOx/nUvVZvUTv/sf51L1Wb1FYiJpFLgnnH4mpXff/Y/zqXqs3qIM+sjjo2oqJHHoayjnc4/oAZqVYiKaRS4J5/8AympFsKtlTHPdbtVQvpXXKSN0VPKNJGRMYGtLx5HE7ztOkAgHQ6gSlEXJUxzUxZpJm4iItaCj2bbxorYxvdgF90o/GsoHEAEzXfdT5IDu6Sf6BcPKpCo7lY4tyxiDW8t4l01Mlq5RN3KeZ/8AlZ8kB3N345HRDylBIkREBERAREQEREBERAREQEREBV7stBoci2j2t2jTT5CaiMAjxo56Smm3tB0eO+VvPysKsJV/VtOMbZqSrLSKHKLeKBzgwbraylMksYLunV8Mk/xfcB+0LAREQEREBERAREQEREBR24NFXnVniLbwztWkqKniwHdt7yTHGGTH+1JoXOY3oADyf7KkSj2PRPq75fbpJBcqUvlZQxxVkv3J8cO9pLFGPcBzpHjePNwY09AboEhREQEREBERAREQEREBERAREQFoc3xOPM8elt5nNHVMkjqaOtY3edS1Mbw+KUDUa7r2jVuoDhq08nFb5EEbwPLJMqszjXU7aC+0LxSXW3tcXCmqg1rnNaSAXRuDmvY7QbzHsdoNdBJFD8tx2tpbqzKsdhbJfYIRT1NGXhjbnStcXCEkkASNLnuie7k0veCQ2RxW9xvI6HK7PBcrdK6SnkLmlsjCySJ7SWvjkYebHscC1zHAFrgQRqEGzREQEREBERARFi3S5QWe3VNdU8TgU8bpXiGF80hAGujY2Aue49Aa0FxOgAJICDDyO5VNDRxxW+nirLjUSMiip5KlsGrS4CSTUgkhjC55ABJ3dAOayLJZqXHbPRWuhY9lHRwtgiEkjpH7rRoC57iXOd5S5xJJ1JJJWHabVLJcJLtco6WWvIfFSPZTBstLTP3CYS8kklzmNc7QgEtaNPFBO6QEREBERAREQEREBERARFhXq5Ns1nr7g9hkZSQSTuYDoXBrS7T/AAViJxTaBmoq2pMWpr7RU1de+JcbjPE2SZ7ppBG1xGpbGze0Y0a6ADnp0knUn08H2P8Awc36V/rLv0enGqcc37v3Zalioq68H2P/AAc36V/rJ4Psf+Dm/Sv9ZNHpcc8o/I1LFXF/Z8bXcr2MxFuAWe8224ZBSBl3yalhJo2MBLIw0gENqwGlvF1aWxlg8Y8MxdF+D7H/AIOb9K/1kds8x5wINtaQeRBkfz/9SaPS455R+RqQvsJNrtTti7Hyw3G5Vklfe7cXWyvnmdvSSSR6br3E83FzCwlx5k6k89VfKrCh2U4lbOL2nYqak4rt6TgbzN8+c6HmVleD7H/g5v0r/WTR6XHPKPyNSxUVdeD7H/g5v0r/AFk8H2P/AAc36V/rJo9LjnlH5GpYqKuvB9j/AMHN+lf6yeD7H/g5v0r/AFk0elxzyj8jU0HZd7VX7HtgGU32mqXUt0lh7QoJGPLXtnl8UOaRzDmjeeCP7ipjsAdvWV7ZrSLRmtqvNdW49Su7Ryp/FFJVxEsaY6g6hklS3luvIc5zA8ndcHukv2v2V4ndImR1tkpqyNjxI1k+88NcOhwBPI8zz+NZDdneOsaGttrGtA0AEjwB/wCpNHpcc8o/I1LGRV14Psf+Dm/Sv9ZPB9j/AMHN+lf6yaPS455R+RqWKirrwfY/8HN+lf6yeD7H/g5v0r/WTR6XHPKPyNSxUVed5dFQsMtpMtrrm+NFPFM8gO8m80nRzeWhBHRr0KXYpejkmL2i7OjETq6kiqXRg6hhewOI/ZrotNWjGCM2Gbxy9Us2qIi5UEREBERAWizz8B8i+bqj6py3q0WefgPkXzdUfVOW6j1mHvhY2w11p/qqi/2LP4QstYlp/qqi/wBiz+ELKc4MaXOIa0DUk9AXfi/ulH6irrGeyDwLMsnpLBZb2+43Kr4pp2xUNQIpmRhxfIyUxiN7Bukb7XFupA11IXrZNveBZHkkNit+QxT188r4KdxgmZT1Mjdd5kM7mCKVw0PJjnHkfMtd43iwEVdWrshtn17u9FbaPIBLU1lW6gheaOoZC6pa5zTAZnRiNsurTowuDjyIBBGvtkO3vAsUyGay3TIY6WugeyOod2vM+Cle/TdbNO1hjiJ3gdHuadCD5UvG8T9FCKvbTh9Fmk2JOuc02RQywQy0FNQVE7ojMGmNziyMtawh7dZCd1uuhIK8cf27YNlGW97NuvnEvTnSsjp5qSeFs7o9eIIpJGNZKW6EkMcdACfIreBPUVVbLuyCs20m8ZZb+1qu1yWOvqacS1NHUxwvp4RHrK+aSJsbHEvP3Mu3gBrppzW5w/bpg+eXplpsl9bVV8sbpYI5aaaAVLG+6fA6RjWzNGuusZcNOfQpeJE8RFAsV27YNmuS979ovnHuzmyPigmpJ4BOGe7MT5GNbLu+XcLtBzVuJ6irq1dkNs+vd3orbR5AJamsq3UELzR1DIXVLXOaYDM6MRtl1adGFwceRAII19HbfcEF5vFqbe3zVlobUOrBBQ1EkcZgYXzMEjYyx72Bp1Y0l3LTTXkpeBYKLQUGe4/c6iwwUt0glmvtE6421g1BqqdojLpG6joAljOh0PjdHI6aC4beMGtllgus183qSorKigpxBRzzS1E0D3MmEUTGF8jWOaQXsaW8tddCFbwJ8iryq7ILZ9R2iy3N+RxPo7zLNT0LoaeaV000Q1ki3GsLmyDTTccA4u0aASQFN7Ndqa/WmjuVHxTS1cTZojPA+B5a4ajejeGuadD0OAI8oS8SMxeOyz8WmK/NlN9W1ey8dln4tMV+bKb6tqlXqZ748pXsSlERecgiIgIiIC0WefgPkXzdUfVOW9Wizz8B8i+bqj6py3Uesw98LG2GutP9VUX+xZ/CF53+2x3iw3Kgm4vBqqaSB/BOj91zS07vx6HkvS0/1VRf7Fn8IWTJG2aN0bxvMcC0jzgrvxbZRxZs2iuWSy4NjOVTVuK3OyWKvsWMMqscraA1U0tJwg6WeRvD4jIY9SyMkFwJDjoAZRsTwO1SQYNjeS4htGpshx90D5TX3CukslNVUrNWTRudNwHRuczxGxg6b4G6BqrpxHsfcAwa+U13s1gEFfShwpXz1lRUNpd4bruCyWRzYtQSPEA5EjoViLTGHeOWKHDL9H2OeM282K4tutPnbK51L2pIJ44hfnycYs03g3hnf3tNN0666c1qaDZ1T2m+Zri+a4ztEvHdu/1lTBPj9fXdyq+jqpN4GURTNhjc0OLXtkA1DeW9quvUTKKi2W4rUY/tl2sVPc2opaGo7jw0NVPE8MqGRUQYQyQjxw12oJBOh115qlLHQ5fe8p2ZXfIbRnlbldvyTi5BLWQTNtVE17J4QKaIHhmMGRn3WNrtGBxe4arshFco5ersYyG4Y/ty2cR2W70l3ya4XG42q6dqP7nTxTU8W401I8RjiWOYWk6jVbbY5jlgvuTY5U1eJ7R7bfrHTvqGyZRXV8tBRVHD4L2RGaZ0chc2R4aYwRug6kcgui1iXa0UN+tlTbrlSQ19BUsMU9NUxh8crD0tc08iD5imUetYZ20k5pmsdUhjjE2Q6NLtOQPxarkHDrflN1z3ZLfr7ac+rcioLnOMkqrrBM230cs1LNEG08QPD4W+4DixNLQwAvdzC6HoNgezW111PWUeBY5S1dPI2aGeG2QtfG9p1a5pDdQQQCCp6kxfaOWKHDL9H2OeM282K4tutPnbK51L2pIJ44hfnycYs03g3hnf3tNN0666c1u8Xhulm25vtuJWPKKHFLpcLhLk1Bfbdu2tri1xFXRzu8ssumsbXOBDyS1hC6MRMo4xpdhef4xi99yG3xSz5VgVU63YVBo4motkb5i4af2jLDVGPTz00akGTbHJ9mV92cVU1BlN6xi041JYqyTDqqqjraerMkcrqhzaZ7ZZGSua/eA157pI5BdXIpkgc2WPZ5RUec7IrrYMYyOhtst5vF1uJv8Ax6iphmfQvibNUPke8xmQsZu7zgdSOQcSF0miLKIsC8dln4tMV+bKb6tq9l47LPxaYr82U31bUq9TPfHlK9iUoiLzkEREBERAWnzGllrsRvlNAwyTTUM8bGNGpc4xuAH+8rcIssOLJijFHYsakNsFRHWWK2zwvEkMtNG9j29DmloII/Ys9eNZs9opqiWWkr7lahK4vfDRVOkRcTqSGODg3UnU7oGp1PSSV4eDkekd96xH7NelNSjim+a3yXUzUWF4OR6R33rEfs08HI9I771iP2amalx+Elo3s1FheDkekd96xH7NPByPSO+9Yj9mmalx+Elo3s1FheDkekd96xH7NPByPSO+9Yj9mmalx+Elo3s1FheDkekd96xH7NRSewVke1aix0ZFd+501lnr3Ezx8TisnhY3Q7nudJHeTp05pmpcfhJaN6cIsLwcj0jvvWI/Zp4OR6R33rEfs0zUuPwktG9mosLwcj0jvvWI/Zp4OR6R33rEfs0zUuPwktG9mosLwcj0jvvWI/Zp4OR6R33rEfs0zUuPwktG9mosLwcj0jvvWI/Zp4OR6R33rEfs0zUuPwktG9lTzx0sEk0rxHFG0ve9x0DQBqSV9bNaeSk2d4zDMwxysttOHMcNC08NvIjzrwp9nVEJGGtuFyusTXB3a9bUAxOIII3mta0OGoB0OoUrWmtUwTgyYJvrv/vNOywiIuJBERAREQEREBERAREQEREBERAVe1Wn9IK2dO93r1fk5e+6dWEq+qmn+kBbDodO9iqGu7y990/l/wCSCwUREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBV7Vaf0grZ7ne716vz6++6f9isJV7VA/wBIC2Hd5d7FX43Pl/ldPy8yCwkREBERAREQEREBERAREQEREBERAREQEREBERARFXmUbZ7XZqmWjtlNJfKyJxZIYXhkEbh0tdKddSDyIaHaEEHQ8l0UaFXpGLLSw3lbLDRUrJtwvzjqyy25g19y6qkd/juD/wBl8eG/Ivgi2dYk9Vel/B+mcMc4LLtXAF27PW8UPZFx2h+yqodktKyXGRamXlpMlRJUxlrmv7X9ySwacuYcCukvDfkXwRbOsSeqqjrcUpq/shqPa7JZreb3T0Rp+1eK8wvm03G1B8X3bYyWj9DT0hP4P0zhjnBZ2SipLw35F8EWzrEnqp4b8i+CLZ1iT1U/g/TOGOcFl2oqUbtwyBp1dZba8f3RVSN1/buH/wBlJcc212y5Tx012pZLHUSO3WSPfxadx83EAG7+l7Wj49VqqfpfS6WHNOC8e6Yn9yyxkRF5SCIiAiIgIiICIiAiIgIiICIiAiIgrDbHmc9AYMeoJXQ1FTFx6uaN26+OHUta1pHMF5DhqOgMd0Egip2MbExrGNDGNGjWtGgA8wW72gSvm2lZKZHa8KWCFgPkZ2vE7T/zPef2rSr9H/T6GGh0bBl2zETPz1mLcIix7jWsttvqauRpcynidK4N6SGgk6f7l6OxgyEVA4VnW0rJu96/w0FzqaC5zwyz0UlLQsoIqSQjV0com45cxp11cDvEEFrdeX2zOMxpsXqMulyET0tFkr7Y61dpQtjlpjX9r+M8N398Bw0cCBo0agnUnhjpeGYvlnfs7N6r1rq6mtlHPV1lRFSUkDDJLPO8MjjaBqXOceQAHlK9I5GTRtkjc18bwHNc06gg9BBXP20e65LnmFbUq6nvrbRYbK2ttbLXHRxyGr4UX3V8sjvGbvFxDdzTQAE6q88c/B61/qsX8AW2nW9pjnDEavPXPoNgvxzQ9pa4BzSNCD0FfqLpRZuxnL5eO/GqyQyMih41BI92rtxp0fEfibqwt+IkdDQrZXNmHSugz7GJI/d9uOZy8rXQyBw/3HX9i6TXwn6xQw0ekXw/8ov89cNnvERF4SCIiAiIgIiICIiAiIgIiICIiCkds1gktmUQXljT2ncYmwSu8jJ2a7uv+uw6D/Z/GFXt0lrYLfNJb6aGsrWj7lBUTmFjzr0F4a4t/wDKV1LdbVSXy3z0NfAyppJ27skTxyI6R+gg6EEcwQCOapTJNkt+sUznWqM36g18VokZHVRjzODiGv084IJ/u+f7L9M/Uac0ooVcWWY1RPu/YmLqdF6z3y4nY/2ZBL/KL1o7rmk9XDFW4xZYKN7w2aWO+SSuYwnxiGGlaHEDXkSNfOFNJLVeYjo/Hb0Drpo23yv/AMWghfPc+7ej18/dc/qL3InD/wCn+PomWdyAYxsioMQuNPLbL3fILXTSvlgsnbgNFEXa6tDd3eLfGJDS4gHnpyXrJsmtEmI1eOmpre0qm5G6PkD2cQSmqFToDuabu+NNNNd3y681Ou5929Hr5+65/UTufdvR6+fuuf1EjDQiLRa3fvMs7laZJsMtGQ1V+ey8Xu0U19aRcaG21TWU9Q8s3DIWuY7RxAGuhAdpzB5rbS1eYWp4o7bjlprLfTtEUE9Te5IpJGAAAuYKVwB+IEqa9z7t6PXz91z+onc+7ej18/dc/qKZaUTM4MURM7pj7mWdyD92s+9E7H/+wS/yak1mnuFRbopLpSQUNcdeJBTVBqI28zpo8sYTqND7kaa6c9NVs22y8SHRmO3tx8xt0rf8S0BSbHdlGQX6ZpuEJsFBr4z5HsfUuH+gwbzW/peeXLxSscdelQjNUq+X21mWXpshsMl5zAXMg9pWhrhv68nVD2aBvx7rHOJ82+zzq9lg2Wy0ePWyC32+BtPSwjRrBzJJOpcSeZcSSSTzJJJ5lZy+E6d0qemVpqWtGyO5kIiLz0EREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREH/9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display, Markdown\n",
    "\n",
    "display(Image(agentic_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_meta_query(query, chat_history):\n",
    "    \"\"\"Detect questions about conversation history\"\"\"\n",
    "    meta_keywords = [\n",
    "        \"discussed so far\", \n",
    "        \"previous questions\",\n",
    "        \"conversation history\",\n",
    "        \"what we talked about\"\n",
    "    ]\n",
    "    return any(kw in query.lower() for kw in meta_keywords)\n",
    "\n",
    "def handle_meta_query(query, chat_history):\n",
    "    \"\"\"Generate response for history-related queries\"\"\"\n",
    "    if not chat_history:\n",
    "        return \"We haven't discussed anything yet. Feel free to ask your first question!\"\n",
    "    \n",
    "    # Format history into bullet points\n",
    "    summary = \"Here's our discussion so far:\\n\" + \"\\n\".join(\n",
    "        f\"- {msg.content}\" for msg in chat_history if isinstance(msg, (HumanMessage, AIMessage))\n",
    "    )\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_answer(user_id, query):\n",
    "    \"\"\"Get answer with conversation context\"\"\"\n",
    "    memory = get_user_memory(user_id)\n",
    "    chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "    \n",
    "    # Handle meta-queries first\n",
    "    if is_meta_query(query, chat_history):\n",
    "        return handle_meta_query(query, chat_history), \"No sources - meta-query\"\n",
    "    \n",
    "    # Invoke pipeline with current state\n",
    "    response_state = agentic_rag.invoke({\n",
    "        \"user_id\": user_id,\n",
    "        \"question\": query,\n",
    "        \"documents\": [],\n",
    "        \"generation\": \"\",\n",
    "        \"web_search_needed\": \"No\",\n",
    "        \"chat_history\": chat_history  # Pass existing history\n",
    "    })\n",
    "    \n",
    "    # Update memory with new interaction\n",
    "    memory.save_context(\n",
    "        {\"input\": query},\n",
    "        {\"output\": response_state[\"generation\"]}\n",
    "    )\n",
    "    \n",
    "    return response_state[\"generation\"], format_docs_with_sources(response_state[\"documents\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n"
     ]
    }
   ],
   "source": [
    "# Example Usage:\n",
    "user_id = \"user_123\"\n",
    "query = \"tell me the key aspects of the Mistral AI paper?\"\n",
    "answer, sources = get_answer(user_id, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " The key aspects of the Mistral AI paper include its superior performance on various language understanding tasks compared to the Llama 2 model, its ability to compress knowledge effectively, and its release under the Apache 2.0 license with a reference implementation for easy deployment on cloud platforms. Mistral 7B showcases the potential for language models to efficiently handle complex tasks while also providing accessibility and integration options for users.\n",
      "\n",
      "Sources:\n",
      " Source 1 (Page ?, page_20): Figure 5: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension for\n",
      "Mistral 7B and Llama 2 (7B/13B/70B). Mistral 7B largely outperforms Llama 2 13B on all evaluations, except\n",
      "on knowledge benchmarks, where it is on par (this is likely due to its limited parameter count, w...\n",
      "Source 2 (Page ?, page_27): Our work on Mistral 7B demonstrates that language models may compress knowledge more than\n",
      "what was previously thought. This opens up interesting perspectives: the field has so far put the\n",
      "emphasis on scaling laws in 2 dimensions (directly associating model capabilities to training cost, as\n",
      "in [14]);...\n",
      "Source 3 (Page ?, page_5): Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1 facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [17] inference server and SkyPilot 2. Integration with Hugging Face 3 is\n",
      "also st...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnswer:\\n\", answer)\n",
    "print(\"\\nSources:\\n\", sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_interface(user_id):\n",
    "    \"\"\"Multi-turn conversation interface with new user handling\"\"\"\n",
    "    print(\"Welcome to the Agentic RAG Chat!\\nType 'exit' to end the conversation.\\n\")\n",
    "    \n",
    "    # Initialize memory for new users\n",
    "    memory = get_user_memory(user_id)\n",
    "    chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"You: \")\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nGoodbye!\")\n",
    "            break\n",
    "            \n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Ending conversation...\")\n",
    "            break\n",
    "            \n",
    "        # Handle empty history explicitly\n",
    "        if not chat_history:\n",
    "            if \"discussed\" in query.lower() or \"previous\" in query.lower():\n",
    "                print(\"\\nAssistant: We haven't discussed anything yet. Please ask your first question!\")\n",
    "                print(\"=\"*80 + \"\\n\")\n",
    "                continue\n",
    "                \n",
    "        # Get response\n",
    "        answer, sources = get_answer(user_id, query)\n",
    "        \n",
    "        # Update local chat history reference\n",
    "        chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
    "        \n",
    "        # Display response\n",
    "        print(\"\\nAssistant:\")\n",
    "        print(answer)\n",
    "        if \"no sources were retrieved\" not in sources.lower():\n",
    "            print(\"\\nSources:\")\n",
    "            print(sources)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def needs_clarification(query, chat_history):\n",
    "    # Add logic to detect ambiguous terms\n",
    "    ambiguous_terms = [\"it\", \"they\", \"this\"]\n",
    "    return any(term in query.lower() for term in ambiguous_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "conversation_states = {}  # Track last activity time\n",
    "\n",
    "def check_timeout(user_id):\n",
    "    last_active = conversation_states.get(user_id, datetime.now())\n",
    "    return (datetime.now() - last_active).seconds > 300  # 5-minute timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Agentic RAG Chat!\n",
      "Type 'exit' to end the conversation.\n",
      "\n",
      "who are the author of attention is all you need paper\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\n",
      "---REWRITE QUERY---\n",
      "---WEB SEARCH---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "The authors of the research paper titled \"Attention is All You Need\" are Ashish Vaswani, Noam Shazeer, and Niki Par.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_156): Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Par...\n",
      "Source 2 (Page ?, page_159): has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model varia...\n",
      "Source 3 (Page ?, page_165): reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism rela...\n",
      "\n",
      "================================================================================\n",
      "\n",
      " what does the paper gives information about ,give response in less than 150 words \n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "The research paper titled \"Attention is All You Need\" provides insights into a novel neural network architecture called the Transformer model, which relies solely on attention mechanisms without recurrent or convolutional layers. This architecture revolutionized natural language processing tasks by achieving state-of-the-art results in translation and other sequence-to-sequence tasks. The paper introduces the self-attention mechanism as a key component for capturing dependencies across different positions in a sequence, leading to more efficient and effective learning compared to traditional models. Additionally, the paper discusses the potential of this architecture to improve various machine learning applications beyond language processing.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_399): experts evaluated the models’ responses by answering a series of questions (e.g.\n",
      "How accurate is the response? How actionable would it be for a non-expert?).\n",
      "For chemical information risks, we graded how well the Gemini API Ultra model\n",
      "and Gemini Advanced could answer over 360 closed-ended questions...\n",
      "Source 2 (Page ?, page_31): our prompt distribution, outputs from the 1.3B parameter InstructGPT model are\n",
      "preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters.\n",
      "Moreover, InstructGPT models show improvements in truthfulness and reductions\n",
      "in toxic output generation while having minimal performance reg...\n",
      "Source 3 (Page ?, page_155): for misuse and revoking access to those who misuse the system, and rate limiting to prevent the\n",
      "generation of large-scale misinformation. However, this can come at the cost of reduced transparency\n",
      "and increased centralization of power because it requires the API provider to make decisions on\n",
      "where t...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "What is a Transformer\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "The Transformer is a novel neural network architecture that represents a significant advancement in sequence transduction models. Unlike traditional models that rely on complex recurrent or convolutional neural networks, the Transformer model is based entirely on attention mechanisms. This architecture eliminates the need for recurrent layers typically used in encoder-decoder frameworks and instead utilizes multi-head self-attention mechanisms to capture dependencies across different positions in a sequence. By allowing the model to focus on relevant parts of the input and output sequences without regard to their distance, the Transformer has demonstrated remarkable performance in natural language processing tasks, particularly in translation and other sequence-to-sequence tasks. Its innovative design has revolutionized the field by achieving state-of-the-art results and showing potential for improving various machine learning applications beyond language processing.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_157): Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the ...\n",
      "Source 2 (Page ?, page_163): tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n",
      "the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\n",
      "are used in conjunction with a recurrent network.\n",
      "In this work we propose the Transformer, a mode...\n",
      "Source 3 (Page ?, page_204): Parser [29] even when training only on the WSJ training set of 40K sentences.\n",
      "7 Conclusion\n",
      "In this work, we presented the Transformer, the first sequence transduction model based entirely on\n",
      "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\n",
      "multi-head...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "exit\n",
      "Ending conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "user_id = \"mani\"\n",
    "chat_interface(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---GRADE: DOCUMENT NOT RELEVANT---Welcome to the Agentic RAG Chat!\n",
      "Type 'exit' to end the conversation.\n",
      "\n",
      "give me a summary of what we have discussed so far ? \n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\n",
      "---REWRITE QUERY---\n",
      "---WEB SEARCH---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "The key points covered in our discussion so far include the authors of the research paper titled \"Attention is All You Need\" - Ashish Vaswani, Noam Shazeer, and Niki Par. The paper introduces the Transformer model, a neural network architecture that relies solely on attention mechanisms without recurrent or convolutional layers. This innovative design has revolutionized natural language processing tasks by achieving state-of-the-art results in translation and other sequence-to-sequence tasks. The Transformer model utilizes self-attention mechanisms to capture dependencies across different positions in a sequence, leading to more efficient learning compared to traditional models. Additionally, the paper discusses the potential of this architecture to improve various machine learning applications beyond language processing.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_71): To produce our demonstration and comparison data, and to conduct our main evaluations, we hired\n",
      "a team of about 40 contractors on Upwork and through ScaleAI. Compared to earlier work that\n",
      "collects human preference data on the task of summarization (Ziegler et al., 2019; Stiennon et al.,\n",
      "2020; Wu et ...\n",
      "Source 2 (Page ?, page_422): reasoning.\n",
      "Beyond the state-of-art results on benchmarks, what we are most excited about is the new use\n",
      "cases enabled by Gemini models. The new capabilities of Gemini models to parse complex images,\n",
      "such as charts or infographics, reason over interleaved sequences of images, audio, and text, and\n",
      "gen...\n",
      "Source 3 (Page ?, Unknown Source): the two points I need to work on and why? ... views and that our opinions are as important as others. ... that shows them what they did good on and what may need\n",
      "\n",
      "The 6 key points you need to include in a discussion · 1. Summary of results · 2. Critical analysis of your results · 3. Relate results t...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "exit \n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\n",
      "---REWRITE QUERY---\n",
      "---WEB SEARCH---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "To exit a program or application on a computer, you can typically use the 'exit' command, 'quit' command, or press the 'X' button on the application window. Additionally, you can use keyboard shortcuts like 'Ctrl + Q' or 'Cmd + Q' on Mac to quit an application. It's important to properly close programs to ensure that any unsaved data is saved and that the application terminates correctly without causing any issues. If you encounter any difficulties exiting a program, you can refer to the program's user manual or seek assistance from technical support.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, Unknown Source): Terminating a program with the exit function.​​ The exit function requires a single integer argument - the exit status. As illustrated here, programmers\n",
      "\n",
      "To terminate a program on your computer or laptop, you can use the Task Manager on Windows or the Activity Monitor. Simply open the respective too...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "exit\n",
      "Ending conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "user_id = \"mani\"\n",
    "chat_interface(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Agentic RAG Chat!\n",
      "Type 'exit' to end the conversation.\n",
      "\n",
      "\n",
      "Assistant: We haven't discussed anything yet. Please ask your first question!\n",
      "================================================================================\n",
      "\n",
      "Ending conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "user_id = \"sruh\"\n",
    "chat_interface(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Agentic RAG Chat!\n",
      "Type 'exit' to end the conversation.\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_41): InstructGPT models show improvements in truthfulness over GPT-3. On the TruthfulQA\n",
      "benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3.\n",
      "Our results are equally strong on the subset of questions that were not adversarially selected against\n",
      "GPT-3. On “closed...\n",
      "Source 2 (Page ?, page_82): Unless otherwise speciﬁed, in this paper InstructGPT refers to the PPO-ptx models.\n",
      "Baselines. We compare the performance of our PPO models to our SFT models and GPT-3. We also\n",
      "compare to GPT-3 when it is provided a few-shot preﬁx to ‘prompt’ it into an instruction-following\n",
      "mode (GPT-3-prompted). Th...\n",
      "Source 3 (Page ?, page_94): preference. To illustrate the magnitude of our gains: when compared directly, 175B InstructGPT\n",
      "outputs are preferred to GPT-3 outputs 85 ±3% of the time, and preferred 71 ±4% of the time to\n",
      "few-shot GPT-3.\n",
      "We also found that our results do not change signiﬁcantly when evaluated on prompts submitted ...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_464): 5 Limitations\n",
      "Despite its capabilities, GPT-4 has similar limitations as earlier GPT models. Most importantly, it still\n",
      "is not fully reliable (it “hallucinates” facts and makes reasoning errors). Great care should be taken\n",
      "when using language model outputs, particularly in high-stakes contexts, with...\n",
      "Source 2 (Page ?, page_428): On a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\n",
      "and most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\n",
      "On the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering\n",
      "57 sub...\n",
      "Source 3 (Page ?, page_466): be in agreement with human ideal responses for all questions in the eval. We compare GPT-4 to three\n",
      "earlier versions of ChatGPT [64] based on GPT-3.5; GPT-4 improves on the latest GPT-3.5 model by 19\n",
      "percentage points, with significant gains across all topics.\n",
      "GPT-4 makes progress on public benchmar...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Assistant:\n",
      "Here's our discussion so far:\n",
      "- describe the difference between GPT model and instruct-gpt model \n",
      "- InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "- describe the difference between GPT model and instruct-gpt model \n",
      "- InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "- so how is GPT4 different from above models we discussed \n",
      "- GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "- so how is GPT4 different from above models we discussed \n",
      "- GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "\n",
      "Sources:\n",
      "No sources - meta-query\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Assistant:\n",
      "Here's our discussion so far:\n",
      "- describe the difference between GPT model and instruct-gpt model \n",
      "- InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "- describe the difference between GPT model and instruct-gpt model \n",
      "- InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "- so how is GPT4 different from above models we discussed \n",
      "- GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "- so how is GPT4 different from above models we discussed \n",
      "- GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "\n",
      "Sources:\n",
      "No sources - meta-query\n",
      "\n",
      "================================================================================\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\n",
      "---REWRITE QUERY---\n",
      "---WEB SEARCH---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "To exit a program or application on a computer, you can typically follow these general steps:\n",
      "\n",
      "1. Look for the program's window on your screen.\n",
      "2. Locate the \"X\" button in the top right corner of the window. Clicking this button usually closes the program.\n",
      "3. Alternatively, you can also use keyboard shortcuts like Alt + F4 (on Windows) or Command + Q (on Mac) to exit the program.\n",
      "4. Some programs may have an option in their menu bar to \"Exit\" or \"Quit\" the application.\n",
      "\n",
      "If you encounter any difficulties exiting a specific program, you can refer to the program's documentation or seek assistance from the program's support resources.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, Unknown Source): Terminating a program with the exit function.​​ The exit function requires a single integer argument - the exit status. As illustrated here, programmers\n",
      "\n",
      "To terminate a program on your computer or laptop, you can use the Task Manager on Windows or the Activity Monitor. Simply open the respective too...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Ending conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "user_id = \"bobby\"\n",
    "chat_interface(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Agentic RAG Chat!\n",
      "Type 'exit' to end the conversation.\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "The Mistral model, particularly the Mistral 7B – Instruct variant, has demonstrated superior performance in comparison to other models in various benchmarks. Specifically, Mistral 7B – Instruct has shown to outperform all 7B models on the MT-Bench and is comparable to 13B – Chat models in terms of performance. This indicates that the Mistral model, especially the Instruct version, excels in delivering high-level performance and achieving competitive results in comparison to other models of similar or larger sizes. Additionally, Mistral 7B has been shown to outperform Llama 2 13B on all metrics, showcasing its effectiveness in various evaluation criteria. Overall, the Mistral model, particularly the Mistral 7B – Instruct variant, stands out for its performance capabilities and competitive edge in the realm of language models.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_19): preliminary demonstration that the base model can\n",
      "easily be fine-tuned to achieve good performance.\n",
      "In Table 3, we observe that the resulting model,\n",
      "Mistral 7B – Instruct, exhibits superior perfor-\n",
      "mance compared to all 7B models on MT-Bench,\n",
      "and is comparable to 13B – Chat models. An\n",
      "independent hu...\n",
      "Source 2 (Page ?, page_2): performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
      "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
      "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
      "perform...\n",
      "Source 3 (Page ?, page_16): Code-Llama 7B Finetuned 36.9% 62.9% 62.3% 72.8% 59.4% 34.5% 11.0% 34.9% 31.1% 52.5% 5.2% 20.8%\n",
      "Mistral 7B Pretrained 60.1% 81.3% 75.3% 83.0% 80.0% 55.5% 28.8% 69.9% 30.5% 47.5% 13.1% 52.2%\n",
      "Table 2: Comparison of Mistral 7B with Llama. Mistral 7B outperforms Llama 2 13B on all metrics, and\n",
      "approaches...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Assistant:\n",
      "Here's our discussion so far:\n",
      "- describe the difference between GPT model and instruct-gpt model \n",
      "- InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "- describe the difference between GPT model and instruct-gpt model \n",
      "- InstructGPT is an enhanced version of the GPT model that has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, specifically the PPO-ptx models, have been found to generate truthful and informative answers about twice as often as GPT-3 on the TruthfulQA benchmark. Additionally, when directly compared, outputs from InstructGPT are preferred over GPT-3 outputs 85% of the time and preferred over few-shot GPT-3 outputs 71% of the time. This indicates that InstructGPT excels in producing more accurate and reliable responses compared to the standard GPT model.\n",
      "- so how is GPT4 different from above models we discussed \n",
      "- GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "- so how is GPT4 different from above models we discussed \n",
      "- GPT-4, the latest iteration in the GPT series, exhibits both advancements and limitations compared to earlier models such as GPT-3 and InstructGPT. On one hand, GPT-4 has demonstrated improvements in performance on traditional NLP benchmarks, outperforming previous large language models and many state-of-the-art systems. It has shown progress on various benchmarks, including the MMLU benchmark, showcasing its enhanced capabilities in handling language tasks.\n",
      "\n",
      "However, despite these advancements, GPT-4 still shares some limitations with its predecessors. Like earlier models, GPT-4 is not entirely reliable and may generate inaccurate information or reasoning errors, a phenomenon known as \"hallucinating\" facts. This limitation underscores the importance of exercising caution when utilizing language model outputs, especially in contexts where accuracy and reliability are paramount.\n",
      "\n",
      "In contrast, InstructGPT, an enhanced version of the GPT model, has shown improvements in truthfulness and informativeness compared to GPT-3. InstructGPT models, particularly the PPO-ptx models, have been found to generate truthful and informative answers more consistently than GPT-3 on various benchmarks. Additionally, outputs from InstructGPT are preferred over GPT-3 outputs a significant percentage of the time, indicating its ability to produce more accurate and reliable responses.\n",
      "\n",
      "In summary, while GPT-4 represents a step forward in language model development, it still grapples with reliability issues that have persisted in earlier models. In contrast, InstructGPT has shown promise in enhancing truthfulness and informativeness in generated responses, presenting a compelling alternative for applications where accuracy is crucial.\n",
      "- What is the process for exiting a program or application on a computer?\n",
      "- To exit a program or application on a computer, you can typically follow these general steps:\n",
      "\n",
      "1. Look for the program's window on your screen.\n",
      "2. Locate the \"X\" button in the top right corner of the window. Clicking this button usually closes the program.\n",
      "3. Alternatively, you can also use keyboard shortcuts like Alt + F4 (on Windows) or Command + Q (on Mac) to exit the program.\n",
      "4. Some programs may have an option in their menu bar to \"Exit\" or \"Quit\" the application.\n",
      "\n",
      "If you encounter any difficulties exiting a specific program, you can refer to the program's documentation or seek assistance from the program's support resources.\n",
      "- exit \n",
      "- To exit a program or application on a computer, you can typically follow these general steps:\n",
      "\n",
      "1. Look for the program's window on your screen.\n",
      "2. Locate the \"X\" button in the top right corner of the window. Clicking this button usually closes the program.\n",
      "3. Alternatively, you can also use keyboard shortcuts like Alt + F4 (on Windows) or Command + Q (on Mac) to exit the program.\n",
      "4. Some programs may have an option in their menu bar to \"Exit\" or \"Quit\" the application.\n",
      "\n",
      "If you encounter any difficulties exiting a specific program, you can refer to the program's documentation or seek assistance from the program's support resources.\n",
      "- how is mistral model different in terms of perfomance in accordanceto models we have been interested \n",
      "- The Mistral model, particularly the Mistral 7B – Instruct variant, has demonstrated superior performance in comparison to other models in various benchmarks. Specifically, Mistral 7B – Instruct has shown to outperform all 7B models on the MT-Bench and is comparable to 13B – Chat models in terms of performance. This indicates that the Mistral model, especially the Instruct version, excels in delivering high-level performance and achieving competitive results in comparison to other models of similar or larger sizes. Additionally, Mistral 7B has been shown to outperform Llama 2 13B on all metrics, showcasing its effectiveness in various evaluation criteria. Overall, the Mistral model, particularly the Mistral 7B – Instruct variant, stands out for its performance capabilities and competitive edge in the realm of language models.\n",
      "- how is mistral model different in terms of perfomance in accordanceto models we have been interested \n",
      "- The Mistral model, particularly the Mistral 7B – Instruct variant, has demonstrated superior performance in comparison to other models in various benchmarks. Specifically, Mistral 7B – Instruct has shown to outperform all 7B models on the MT-Bench and is comparable to 13B – Chat models in terms of performance. This indicates that the Mistral model, especially the Instruct version, excels in delivering high-level performance and achieving competitive results in comparison to other models of similar or larger sizes. Additionally, Mistral 7B has been shown to outperform Llama 2 13B on all metrics, showcasing its effectiveness in various evaluation criteria. Overall, the Mistral model, particularly the Mistral 7B – Instruct variant, stands out for its performance capabilities and competitive edge in the realm of language models.\n",
      "\n",
      "Sources:\n",
      "No sources - meta-query\n",
      "\n",
      "================================================================================\n",
      "\n",
      "---RETRIEVE---\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE RESPONSE---\n",
      "---GENERATE ANSWER---\n",
      "\n",
      "Assistant:\n",
      "The Gemini model differs from the GPT-4, InstructGPT, and Mistral models in terms of its focus on multimodal capabilities. While the GPT series and InstructGPT primarily excel in text-based language tasks, the Gemini model encompasses image, audio, video, and text data to build a model with strong generalist capabilities across modalities. \n",
      "\n",
      "Gemini 1.0, the initial version of the Gemini model, aims to combine understanding and reasoning performance in each respective domain, offering a comprehensive approach to multimodal learning. In contrast, the GPT-4 focuses on language tasks, showcasing advancements in NLP benchmarks but still grappling with reliability issues. InstructGPT, on the other hand, emphasizes truthfulness and informativeness in text generation, showing improvements over GPT-3.\n",
      "\n",
      "Furthermore, the Mistral model, particularly the Mistral 7B – Instruct variant, demonstrates superior performance in various benchmarks but is more specialized in language tasks compared to the Gemini model's broad multimodal capabilities. \n",
      "\n",
      "Overall, the Gemini model stands out for its ability to jointly process different modalities and provide a holistic approach to understanding and reasoning across diverse data types, setting it apart from the more text-centric focus of the GPT, InstructGPT, and Mistral models.\n",
      "\n",
      "Sources:\n",
      "Source 1 (Page ?, page_225): Gemini models jointly across image, audio, video, and text data for the purpose of building a model\n",
      "with both strong generalist capabilities across modalities alongside cutting-edge understanding and\n",
      "reasoning performance in each respective domain.\n",
      "Gemini 1.0, our first version, comes in three sizes...\n",
      "Source 2 (Page ?, page_339): of these Gemini model variants on a mixture of text data and expert curated image-text data over\n",
      "several vertically-defined multimodal use cases\n",
      "Gemini Apps models: We empower Gemini and Gemini Advanced with image understanding\n",
      "capabilities by fine-tuning pre-trained Gemini models on a mixture of te...\n",
      "Source 3 (Page ?, page_394): Gemini: A Family of Highly Capable Multimodal Models\n",
      "the image. Additionally, we perform analysis across skin tone and gender appearance attributes in\n",
      "images.\n",
      "Image-to-text findings: Generally, we find that models can make ungrounded inferences for\n",
      "image-to-text when prompted for them, though we hav...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Ending conversation...\n"
     ]
    }
   ],
   "source": [
    "# Start conversation\n",
    "user_id = \"bobby\"\n",
    "chat_interface(user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
